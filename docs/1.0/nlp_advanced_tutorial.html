
<!DOCTYPE HTML>
<html lang="zh-hans" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>Advanced: Making Dynamic Decisions and the Bi-LSTM CRF · Pytorch 中文文档</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.3">
        <meta name="author" content="ApacheCN">
        
        
    
    <link rel="stylesheet" href="gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-insert-logo/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-back-to-top-button/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-code/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-pageview-count/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-emphasize/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-alerts/style.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-auto-scroll-table/website.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-page-toc-button/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-tbfed-pagefooter/footer.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="seq2seq_translation_tutorial.html" />
    
    
    <link rel="prev" href="nlp_sequence_models_tutorial.html" />
    

    <style>
    @media only screen and (max-width: 640px) {
        .book-header .hidden-mobile {
            display: none;
        }
    }
    </style>
    <script>
        window["gitbook-plugin-github-buttons"] = {};
    </script>

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="输入并搜索" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="./">
            
                <a href="./">
            
                    
                    Introduction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" >
            
                <span>
            
                    
                    中文教程
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.1" data-path="tut_getting_started.html">
            
                <a href="tut_getting_started.html">
            
                    
                    起步
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.1.1" data-path="deep_learning_60min_blitz.html">
            
                <a href="deep_learning_60min_blitz.html">
            
                    
                    PyTorch 深度学习: 60 分钟极速入门
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.1.1.1" data-path="blitz_tensor_tutorial.html">
            
                <a href="blitz_tensor_tutorial.html">
            
                    
                    什么是 PyTorch？
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.1.2" data-path="blitz_autograd_tutorial.html">
            
                <a href="blitz_autograd_tutorial.html">
            
                    
                    Autograd：自动求导
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.1.3" data-path="blitz_neural_networks_tutorial.html">
            
                <a href="blitz_neural_networks_tutorial.html">
            
                    
                    神经网络
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.1.4" data-path="blitz_cifar10_tutorial.html">
            
                <a href="blitz_cifar10_tutorial.html">
            
                    
                    训练分类器
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.1.5" data-path="blitz_data_parallel_tutorial.html">
            
                <a href="blitz_data_parallel_tutorial.html">
            
                    
                    可选：数据并行处理
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.1.2" data-path="data_loading_tutorial.html">
            
                <a href="data_loading_tutorial.html">
            
                    
                    数据加载和处理教程
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.3" data-path="pytorch_with_examples.html">
            
                <a href="pytorch_with_examples.html">
            
                    
                    用例子学习 PyTorch
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.4" data-path="transfer_learning_tutorial.html">
            
                <a href="transfer_learning_tutorial.html">
            
                    
                    迁移学习教程
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.5" data-path="deploy_seq2seq_hybrid_frontend_tutorial.html">
            
                <a href="deploy_seq2seq_hybrid_frontend_tutorial.html">
            
                    
                    混合前端的 seq2seq 模型部署
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.6" data-path="saving_loading_models.html">
            
                <a href="saving_loading_models.html">
            
                    
                    Saving and Loading Models
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.7" data-path="nn_tutorial.html">
            
                <a href="nn_tutorial.html">
            
                    
                    What is torch.nn really?
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.2" data-path="tut_image.html">
            
                <a href="tut_image.html">
            
                    
                    图像
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.2.1" data-path="finetuning_torchvision_models_tutorial.html">
            
                <a href="finetuning_torchvision_models_tutorial.html">
            
                    
                    Torchvision 模型微调
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2.2" data-path="spatial_transformer_tutorial.html">
            
                <a href="spatial_transformer_tutorial.html">
            
                    
                    空间变换器网络教程
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2.3" data-path="neural_style_tutorial.html">
            
                <a href="neural_style_tutorial.html">
            
                    
                    使用 PyTorch 进行图像风格转换
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2.4" data-path="fgsm_tutorial.html">
            
                <a href="fgsm_tutorial.html">
            
                    
                    对抗性示例生成
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2.5" data-path="super_resolution_with_caffe2.html">
            
                <a href="super_resolution_with_caffe2.html">
            
                    
                    使用 ONNX 将模型从 PyTorch 传输到 Caffe2 和移动端
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.3" data-path="tut_text.html">
            
                <a href="tut_text.html">
            
                    
                    文本
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.3.1" data-path="chatbot_tutorial.html">
            
                <a href="chatbot_tutorial.html">
            
                    
                    聊天机器人教程
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.3.2" data-path="char_rnn_generation_tutorial.html">
            
                <a href="char_rnn_generation_tutorial.html">
            
                    
                    使用字符级别特征的 RNN 网络生成姓氏
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.3.3" data-path="char_rnn_classification_tutorial.html">
            
                <a href="char_rnn_classification_tutorial.html">
            
                    
                    使用字符级别特征的 RNN 网络进行姓氏分类
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.3.4" data-path="deep_learning_nlp_tutorial.html">
            
                <a href="deep_learning_nlp_tutorial.html">
            
                    
                    Deep Learning for NLP with Pytorch
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.3.4.1" data-path="nlp_pytorch_tutorial.html">
            
                <a href="nlp_pytorch_tutorial.html">
            
                    
                    PyTorch 介绍
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.3.4.2" data-path="nlp_deep_learning_tutorial.html">
            
                <a href="nlp_deep_learning_tutorial.html">
            
                    
                    使用 PyTorch 进行深度学习
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.3.4.3" data-path="nlp_word_embeddings_tutorial.html">
            
                <a href="nlp_word_embeddings_tutorial.html">
            
                    
                    Word Embeddings: Encoding Lexical Semantics
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.3.4.4" data-path="nlp_sequence_models_tutorial.html">
            
                <a href="nlp_sequence_models_tutorial.html">
            
                    
                    序列模型和 LSTM 网络
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="1.2.3.4.5" data-path="nlp_advanced_tutorial.html">
            
                <a href="nlp_advanced_tutorial.html">
            
                    
                    Advanced: Making Dynamic Decisions and the Bi-LSTM CRF
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.3.5" data-path="seq2seq_translation_tutorial.html">
            
                <a href="seq2seq_translation_tutorial.html">
            
                    
                    基于注意力机制的 seq2seq 神经网络翻译
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.4" data-path="tut_generative.html">
            
                <a href="tut_generative.html">
            
                    
                    生成
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.4.1" data-path="dcgan_faces_tutorial.html">
            
                <a href="dcgan_faces_tutorial.html">
            
                    
                    DCGAN Tutorial
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.5" data-path="tut_reinforcement_learning.html">
            
                <a href="tut_reinforcement_learning.html">
            
                    
                    强化学习
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.5.1" data-path="reinforcement_q_learning.html">
            
                <a href="reinforcement_q_learning.html">
            
                    
                    Reinforcement Learning (DQN) Tutorial
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.6" data-path="tut_extending_pytorch.html">
            
                <a href="tut_extending_pytorch.html">
            
                    
                    扩展 PyTorch
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.6.1" data-path="numpy_extensions_tutorial.html">
            
                <a href="numpy_extensions_tutorial.html">
            
                    
                    用 numpy 和 scipy 创建扩展
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.6.2" data-path="cpp_extension.html">
            
                <a href="cpp_extension.html">
            
                    
                    Custom C++   and CUDA Extensions
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.6.3" data-path="torch_script_custom_ops.html">
            
                <a href="torch_script_custom_ops.html">
            
                    
                    Extending TorchScript with Custom C++   Operators
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.7" data-path="tut_production_usage.html">
            
                <a href="tut_production_usage.html">
            
                    
                    生产性使用
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.7.1" data-path="dist_tuto.html">
            
                <a href="dist_tuto.html">
            
                    
                    Writing Distributed Applications with PyTorch
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7.2" data-path="aws_distributed_training_tutorial.html">
            
                <a href="aws_distributed_training_tutorial.html">
            
                    
                    使用 Amazon AWS 进行分布式训练
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7.3" data-path="ONNXLive.html">
            
                <a href="ONNXLive.html">
            
                    
                    ONNX 现场演示教程
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7.4" data-path="cpp_export.html">
            
                <a href="cpp_export.html">
            
                    
                    在 C++ 中加载 PYTORCH 模型
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.8" data-path="tut_other_language.html">
            
                <a href="tut_other_language.html">
            
                    
                    其它语言中的 PyTorch
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.8.1" data-path="cpp_frontend.html">
            
                <a href="cpp_frontend.html">
            
                    
                    使用 PyTorch C++ 前端
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.3" >
            
                <span>
            
                    
                    中文文档
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.1" data-path="docs_notes.html">
            
                <a href="docs_notes.html">
            
                    
                    注解
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.1.1" data-path="notes_autograd.html">
            
                <a href="notes_autograd.html">
            
                    
                    自动求导机制
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.2" data-path="notes_broadcasting.html">
            
                <a href="notes_broadcasting.html">
            
                    
                    广播语义
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.3" data-path="notes_cuda.html">
            
                <a href="notes_cuda.html">
            
                    
                    CUDA 语义
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.4" data-path="notes_extending.html">
            
                <a href="notes_extending.html">
            
                    
                    Extending PyTorch
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.5" data-path="notes_faq.html">
            
                <a href="notes_faq.html">
            
                    
                    Frequently Asked Questions
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.6" data-path="notes_multiprocessing.html">
            
                <a href="notes_multiprocessing.html">
            
                    
                    Multiprocessing best practices
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.7" data-path="notes_randomness.html">
            
                <a href="notes_randomness.html">
            
                    
                    Reproducibility
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.8" data-path="notes_serialization.html">
            
                <a href="notes_serialization.html">
            
                    
                    Serialization semantics
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.9" data-path="notes_windows.html">
            
                <a href="notes_windows.html">
            
                    
                    Windows FAQ
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.3.2" data-path="docs_package_ref.html">
            
                <a href="docs_package_ref.html">
            
                    
                    包参考
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.2.1" data-path="torch.html">
            
                <a href="torch.html">
            
                    
                    torch
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.2" data-path="tensors.html">
            
                <a href="tensors.html">
            
                    
                    torch.Tensor
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.3" data-path="tensor_attributes.html">
            
                <a href="tensor_attributes.html">
            
                    
                    Tensor Attributes
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.4" data-path="type_info.html">
            
                <a href="type_info.html">
            
                    
                    数据类型信息
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.5" data-path="sparse.html">
            
                <a href="sparse.html">
            
                    
                    torch.sparse
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.6" data-path="cuda.html">
            
                <a href="cuda.html">
            
                    
                    torch.cuda
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.7" data-path="storage.html">
            
                <a href="storage.html">
            
                    
                    torch.Storage
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.8" data-path="nn.html">
            
                <a href="nn.html">
            
                    
                    torch.nn
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.9" data-path="nn_functional.html">
            
                <a href="nn_functional.html">
            
                    
                    torch.nn.functional
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.10" data-path="nn_init.html">
            
                <a href="nn_init.html">
            
                    
                    torch.nn.init
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.11" data-path="optim.html">
            
                <a href="optim.html">
            
                    
                    torch.optim
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.12" data-path="autograd.html">
            
                <a href="autograd.html">
            
                    
                    Automatic differentiation package - torch.autograd
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.13" data-path="distributed.html">
            
                <a href="distributed.html">
            
                    
                    Distributed communication package - torch.distributed
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.14" data-path="distributions.html">
            
                <a href="distributions.html">
            
                    
                    Probability distributions - torch.distributions
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.15" data-path="jit.html">
            
                <a href="jit.html">
            
                    
                    Torch Script
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.16" data-path="multiprocessing.html">
            
                <a href="multiprocessing.html">
            
                    
                    多进程包 - torch.multiprocessing
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.17" data-path="bottleneck.html">
            
                <a href="bottleneck.html">
            
                    
                    torch.utils.bottleneck
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.18" data-path="checkpoint.html">
            
                <a href="checkpoint.html">
            
                    
                    torch.utils.checkpoint
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.19" data-path="docs_cpp_extension.html">
            
                <a href="docs_cpp_extension.html">
            
                    
                    torch.utils.cpp_extension
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.20" data-path="data.html">
            
                <a href="data.html">
            
                    
                    torch.utils.data
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.21" data-path="dlpack.html">
            
                <a href="dlpack.html">
            
                    
                    torch.utils.dlpack
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.22" data-path="hub.html">
            
                <a href="hub.html">
            
                    
                    torch.hub
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.23" data-path="model_zoo.html">
            
                <a href="model_zoo.html">
            
                    
                    torch.utils.model_zoo
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.24" data-path="onnx.html">
            
                <a href="onnx.html">
            
                    
                    torch.onnx
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.25" data-path="distributed_deprecated.html">
            
                <a href="distributed_deprecated.html">
            
                    
                    Distributed communication package (deprecated) - torch.distributed.deprecated
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.3.3" data-path="docs_torchvision_ref.html">
            
                <a href="docs_torchvision_ref.html">
            
                    
                    torchvision 参考
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.3.1" data-path="torchvision_datasets.html">
            
                <a href="torchvision_datasets.html">
            
                    
                    torchvision.datasets
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.2" data-path="torchvision_models.html">
            
                <a href="torchvision_models.html">
            
                    
                    torchvision.models
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.3" data-path="torchvision_transforms.html">
            
                <a href="torchvision_transforms.html">
            
                    
                    torchvision.transforms
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.4" data-path="torchvision_utils.html">
            
                <a href="torchvision_utils.html">
            
                    
                    torchvision.utils
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

            </ul>
            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            本书使用 GitBook 发布
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href="." >Advanced: Making Dynamic Decisions and the Bi-LSTM CRF</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h1 id="advanced-making-dynamic-decisions-and-the-bi-lstm-crf">Advanced: Making Dynamic Decisions and the Bi-LSTM CRF</h1>
<h2 id="dynamic-versus-static-deep-learning-toolkits">Dynamic versus Static Deep Learning Toolkits</h2>
<p>Pytorch is a <em>dynamic</em> neural network kit. Another example of a dynamic kit is <a href="https://github.com/clab/dynet" target="_blank">Dynet</a> (I mention this because working with Pytorch and Dynet is similar. If you see an example in Dynet, it will probably help you implement it in Pytorch). The opposite is the <em>static</em> tool kit, which includes Theano, Keras, TensorFlow, etc. The core difference is the following:</p>
<ul>
<li>In a static toolkit, you define a computation graph once, compile it, and then stream instances to it.</li>
<li>In a dynamic toolkit, you define a computation graph <em>for each instance</em>. It is never compiled and is executed on-the-fly</li>
</ul>
<p>Without a lot of experience, it is difficult to appreciate the difference. One example is to suppose we want to build a deep constituent parser. Suppose our model involves roughly the following steps:</p>
<ul>
<li>We build the tree bottom up</li>
<li>Tag the root nodes (the words of the sentence)</li>
<li>From there, use a neural network and the embeddings of the words to find combinations that form constituents. Whenever you form a new constituent, use some sort of technique to get an embedding of the constituent. In this case, our network architecture will depend completely on the input sentence. In the sentence &#x201C;The green cat scratched the wall&#x201D;, at some point in the model, we will want to combine the span <code>\((i,j,r) = (1, 3, \text{NP})\)</code> (that is, an NP constituent spans word 1 to word 3, in this case &#x201C;The green cat&#x201D;).</li>
</ul>
<p>However, another sentence might be &#x201C;Somewhere, the big fat cat scratched the wall&#x201D;. In this sentence, we will want to form the constituent <code>\((2, 4, NP)\)</code> at some point. The constituents we will want to form will depend on the instance. If we just compile the computation graph once, as in a static toolkit, it will be exceptionally difficult or impossible to program this logic. In a dynamic toolkit though, there isn&#x2019;t just 1 pre-defined computation graph. There can be a new computation graph for each instance, so this problem goes away.</p>
<p>Dynamic toolkits also have the advantage of being easier to debug and the code more closely resembling the host language (by that I mean that Pytorch and Dynet look more like actual Python code than Keras or Theano).</p>
<h2 id="bi-lstm-conditional-random-field-discussion">Bi-LSTM Conditional Random Field Discussion</h2>
<p>For this section, we will see a full, complicated example of a Bi-LSTM Conditional Random Field for named-entity recognition. The LSTM tagger above is typically sufficient for part-of-speech tagging, but a sequence model like the CRF is really essential for strong performance on NER. Familiarity with CRF&#x2019;s is assumed. Although this name sounds scary, all the model is is a CRF but where an LSTM provides the features. This is an advanced model though, far more complicated than any earlier model in this tutorial. If you want to skip it, that is fine. To see if you&#x2019;re ready, see if you can:</p>
<ul>
<li>Write the recurrence for the viterbi variable at step i for tag k.</li>
<li>Modify the above recurrence to compute the forward variables instead.</li>
<li>Modify again the above recurrence to compute the forward variables in log-space (hint: log-sum-exp)</li>
</ul>
<p>If you can do those three things, you should be able to understand the code below. Recall that the CRF computes a conditional probability. Let <code>\(y\)</code> be a tag sequence and <code>\(x\)</code> an input sequence of words. Then we compute</p>
<p><script type="math/tex; ">P(y|x) = \frac{\exp{(\text{Score}(x, y)})}{\sum_{y'} \exp{(\text{Score}(x, y')})}</script></p>
<p>Where the score is determined by defining some log potentials <code>\(\log \psi_i(x,y)\)</code> such that</p>
<p><script type="math/tex; ">\text{Score}(x,y) = \sum_i \log \psi_i(x,y)</script></p>
<p>To make the partition function tractable, the potentials must look only at local features.</p>
<p>In the Bi-LSTM CRF, we define two kinds of potentials: emission and transition. The emission potential for the word at index <code>\(i\)</code> comes from the hidden state of the Bi-LSTM at timestep <code>\(i\)</code>. The transition scores are stored in a <code>\(|T|x|T|\)</code> matrix <code>\(\textbf{P}\)</code>, where <code>\(T\)</code> is the tag set. In my implementation, <code>\(\textbf{P}_{j,k}\)</code> is the score of transitioning to tag <code>\(j\)</code> from tag <code>\(k\)</code>. So:</p>
<p><script type="math/tex; ">\text{Score}(x,y) = \sum_i \log \psi_\text{EMIT}(y_i \rightarrow x_i) + \log \psi_\text{TRANS}(y_{i-1} \rightarrow y_i)</script></p>
<p><script type="math/tex; ">= \sum_i h_i[y_i] + \textbf{P}_{y_i, y_{i-1}}</script></p>
<p>where in this second expression, we think of the tags as being assigned unique non-negative indices.</p>
<p>If the above discussion was too brief, you can check out <a href="http://www.cs.columbia.edu/%7Emcollins/crf.pdf" target="_blank">this</a> write up from Michael Collins on CRFs.</p>
<h2 id="implementation-notes">Implementation Notes</h2>
<p>The example below implements the forward algorithm in log space to compute the partition function, and the viterbi algorithm to decode. Backpropagation will compute the gradients automatically for us. We don&#x2019;t have to do anything by hand.</p>
<p>The implementation is not optimized. If you understand what is going on, you&#x2019;ll probably quickly see that iterating over the next tag in the forward algorithm could probably be done in one big operation. I wanted to code to be more readable. If you want to make the relevant change, you could probably use this tagger for real tasks.</p>
<pre><code class="lang-py"><span class="hljs-comment"># Author: Robert Guthrie</span>

<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torch.autograd <span class="hljs-keyword">as</span> autograd
<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn
<span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim

torch.manual_seed(<span class="hljs-number">1</span>)
</code></pre>
<p>Helper functions to make the code more readable.</p>
<pre><code class="lang-py"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">argmax</span><span class="hljs-params">(vec)</span>:</span>
    <span class="hljs-comment"># return the argmax as a python int</span>
    _, idx = torch.max(vec, <span class="hljs-number">1</span>)
    <span class="hljs-keyword">return</span> idx.item()

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">prepare_sequence</span><span class="hljs-params">(seq, to_ix)</span>:</span>
    idxs = [to_ix[w] <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> seq]
    <span class="hljs-keyword">return</span> torch.tensor(idxs, dtype=torch.long)

<span class="hljs-comment"># Compute log sum exp in a numerically stable way for the forward algorithm</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">log_sum_exp</span><span class="hljs-params">(vec)</span>:</span>
    max_score = vec[<span class="hljs-number">0</span>, argmax(vec)]
    max_score_broadcast = max_score.view(<span class="hljs-number">1</span>, <span class="hljs-number">-1</span>).expand(<span class="hljs-number">1</span>, vec.size()[<span class="hljs-number">1</span>])
    <span class="hljs-keyword">return</span> max_score + \
        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))
</code></pre>
<p>Create model</p>
<pre><code class="lang-py"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">BiLSTM_CRF</span><span class="hljs-params">(nn.Module)</span>:</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim)</span>:</span>
        super(BiLSTM_CRF, self).__init__()
        self.embedding_dim = embedding_dim
        self.hidden_dim = hidden_dim
        self.vocab_size = vocab_size
        self.tag_to_ix = tag_to_ix
        self.tagset_size = len(tag_to_ix)

        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim // <span class="hljs-number">2</span>,
                            num_layers=<span class="hljs-number">1</span>, bidirectional=<span class="hljs-keyword">True</span>)

        <span class="hljs-comment"># Maps the output of the LSTM into tag space.</span>
        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)

        <span class="hljs-comment"># Matrix of transition parameters.  Entry i,j is the score of</span>
        <span class="hljs-comment"># transitioning *to* i *from* j.</span>
        self.transitions = nn.Parameter(
            torch.randn(self.tagset_size, self.tagset_size))

        <span class="hljs-comment"># These two statements enforce the constraint that we never transfer</span>
        <span class="hljs-comment"># to the start tag and we never transfer from the stop tag</span>
        self.transitions.data[tag_to_ix[START_TAG], :] = <span class="hljs-number">-10000</span>
        self.transitions.data[:, tag_to_ix[STOP_TAG]] = <span class="hljs-number">-10000</span>

        self.hidden = self.init_hidden()

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">init_hidden</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-keyword">return</span> (torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, self.hidden_dim // <span class="hljs-number">2</span>),
                torch.randn(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, self.hidden_dim // <span class="hljs-number">2</span>))

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_forward_alg</span><span class="hljs-params">(self, feats)</span>:</span>
        <span class="hljs-comment"># Do the forward algorithm to compute the partition function</span>
        init_alphas = torch.full((<span class="hljs-number">1</span>, self.tagset_size), <span class="hljs-number">-10000.</span>)
        <span class="hljs-comment"># START_TAG has all of the score.</span>
        init_alphas[<span class="hljs-number">0</span>][self.tag_to_ix[START_TAG]] = <span class="hljs-number">0.</span>

        <span class="hljs-comment"># Wrap in a variable so that we will get automatic backprop</span>
        forward_var = init_alphas

        <span class="hljs-comment"># Iterate through the sentence</span>
        <span class="hljs-keyword">for</span> feat <span class="hljs-keyword">in</span> feats:
            alphas_t = []  <span class="hljs-comment"># The forward tensors at this timestep</span>
            <span class="hljs-keyword">for</span> next_tag <span class="hljs-keyword">in</span> range(self.tagset_size):
                <span class="hljs-comment"># broadcast the emission score: it is the same regardless of</span>
                <span class="hljs-comment"># the previous tag</span>
                emit_score = feat[next_tag].view(
                    <span class="hljs-number">1</span>, <span class="hljs-number">-1</span>).expand(<span class="hljs-number">1</span>, self.tagset_size)
                <span class="hljs-comment"># the ith entry of trans_score is the score of transitioning to</span>
                <span class="hljs-comment"># next_tag from i</span>
                trans_score = self.transitions[next_tag].view(<span class="hljs-number">1</span>, <span class="hljs-number">-1</span>)
                <span class="hljs-comment"># The ith entry of next_tag_var is the value for the</span>
                <span class="hljs-comment"># edge (i -&gt; next_tag) before we do log-sum-exp</span>
                next_tag_var = forward_var + trans_score + emit_score
                <span class="hljs-comment"># The forward variable for this tag is log-sum-exp of all the</span>
                <span class="hljs-comment"># scores.</span>
                alphas_t.append(log_sum_exp(next_tag_var).view(<span class="hljs-number">1</span>))
            forward_var = torch.cat(alphas_t).view(<span class="hljs-number">1</span>, <span class="hljs-number">-1</span>)
        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]
        alpha = log_sum_exp(terminal_var)
        <span class="hljs-keyword">return</span> alpha

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_get_lstm_features</span><span class="hljs-params">(self, sentence)</span>:</span>
        self.hidden = self.init_hidden()
        embeds = self.word_embeds(sentence).view(len(sentence), <span class="hljs-number">1</span>, <span class="hljs-number">-1</span>)
        lstm_out, self.hidden = self.lstm(embeds, self.hidden)
        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)
        lstm_feats = self.hidden2tag(lstm_out)
        <span class="hljs-keyword">return</span> lstm_feats

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_score_sentence</span><span class="hljs-params">(self, feats, tags)</span>:</span>
        <span class="hljs-comment"># Gives the score of a provided tag sequence</span>
        score = torch.zeros(<span class="hljs-number">1</span>)
        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags])
        <span class="hljs-keyword">for</span> i, feat <span class="hljs-keyword">in</span> enumerate(feats):
            score = score + \
                self.transitions[tags[i + <span class="hljs-number">1</span>], tags[i]] + feat[tags[i + <span class="hljs-number">1</span>]]
        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[<span class="hljs-number">-1</span>]]
        <span class="hljs-keyword">return</span> score

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_viterbi_decode</span><span class="hljs-params">(self, feats)</span>:</span>
        backpointers = []

        <span class="hljs-comment"># Initialize the viterbi variables in log space</span>
        init_vvars = torch.full((<span class="hljs-number">1</span>, self.tagset_size), <span class="hljs-number">-10000.</span>)
        init_vvars[<span class="hljs-number">0</span>][self.tag_to_ix[START_TAG]] = <span class="hljs-number">0</span>

        <span class="hljs-comment"># forward_var at step i holds the viterbi variables for step i-1</span>
        forward_var = init_vvars
        <span class="hljs-keyword">for</span> feat <span class="hljs-keyword">in</span> feats:
            bptrs_t = []  <span class="hljs-comment"># holds the backpointers for this step</span>
            viterbivars_t = []  <span class="hljs-comment"># holds the viterbi variables for this step</span>

            <span class="hljs-keyword">for</span> next_tag <span class="hljs-keyword">in</span> range(self.tagset_size):
                <span class="hljs-comment"># next_tag_var[i] holds the viterbi variable for tag i at the</span>
                <span class="hljs-comment"># previous step, plus the score of transitioning</span>
                <span class="hljs-comment"># from tag i to next_tag.</span>
                <span class="hljs-comment"># We don&apos;t include the emission scores here because the max</span>
                <span class="hljs-comment"># does not depend on them (we add them in below)</span>
                next_tag_var = forward_var + self.transitions[next_tag]
                best_tag_id = argmax(next_tag_var)
                bptrs_t.append(best_tag_id)
                viterbivars_t.append(next_tag_var[<span class="hljs-number">0</span>][best_tag_id].view(<span class="hljs-number">1</span>))
            <span class="hljs-comment"># Now add in the emission scores, and assign forward_var to the set</span>
            <span class="hljs-comment"># of viterbi variables we just computed</span>
            forward_var = (torch.cat(viterbivars_t) + feat).view(<span class="hljs-number">1</span>, <span class="hljs-number">-1</span>)
            backpointers.append(bptrs_t)

        <span class="hljs-comment"># Transition to STOP_TAG</span>
        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]
        best_tag_id = argmax(terminal_var)
        path_score = terminal_var[<span class="hljs-number">0</span>][best_tag_id]

        <span class="hljs-comment"># Follow the back pointers to decode the best path.</span>
        best_path = [best_tag_id]
        <span class="hljs-keyword">for</span> bptrs_t <span class="hljs-keyword">in</span> reversed(backpointers):
            best_tag_id = bptrs_t[best_tag_id]
            best_path.append(best_tag_id)
        <span class="hljs-comment"># Pop off the start tag (we dont want to return that to the caller)</span>
        start = best_path.pop()
        <span class="hljs-keyword">assert</span> start == self.tag_to_ix[START_TAG]  <span class="hljs-comment"># Sanity check</span>
        best_path.reverse()
        <span class="hljs-keyword">return</span> path_score, best_path

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">neg_log_likelihood</span><span class="hljs-params">(self, sentence, tags)</span>:</span>
        feats = self._get_lstm_features(sentence)
        forward_score = self._forward_alg(feats)
        gold_score = self._score_sentence(feats, tags)
        <span class="hljs-keyword">return</span> forward_score - gold_score

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, sentence)</span>:</span>  <span class="hljs-comment"># dont confuse this with _forward_alg above.</span>
        <span class="hljs-comment"># Get the emission scores from the BiLSTM</span>
        lstm_feats = self._get_lstm_features(sentence)

        <span class="hljs-comment"># Find the best path, given the features.</span>
        score, tag_seq = self._viterbi_decode(lstm_feats)
        <span class="hljs-keyword">return</span> score, tag_seq
</code></pre>
<p>Run training</p>
<pre><code class="lang-py">START_TAG = <span class="hljs-string">&quot;&lt;START&gt;&quot;</span>
STOP_TAG = <span class="hljs-string">&quot;&lt;STOP&gt;&quot;</span>
EMBEDDING_DIM = <span class="hljs-number">5</span>
HIDDEN_DIM = <span class="hljs-number">4</span>

<span class="hljs-comment"># Make up some training data</span>
training_data = [(
    <span class="hljs-string">&quot;the wall street journal reported today that apple corporation made money&quot;</span>.split(),
    <span class="hljs-string">&quot;B I I I O O O B I O O&quot;</span>.split()
), (
    <span class="hljs-string">&quot;georgia tech is a university in georgia&quot;</span>.split(),
    <span class="hljs-string">&quot;B I O O O O B&quot;</span>.split()
)]

word_to_ix = {}
<span class="hljs-keyword">for</span> sentence, tags <span class="hljs-keyword">in</span> training_data:
    <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> sentence:
        <span class="hljs-keyword">if</span> word <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> word_to_ix:
            word_to_ix[word] = len(word_to_ix)

tag_to_ix = {<span class="hljs-string">&quot;B&quot;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&quot;I&quot;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&quot;O&quot;</span>: <span class="hljs-number">2</span>, START_TAG: <span class="hljs-number">3</span>, STOP_TAG: <span class="hljs-number">4</span>}

model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)
optimizer = optim.SGD(model.parameters(), lr=<span class="hljs-number">0.01</span>, weight_decay=<span class="hljs-number">1e-4</span>)

<span class="hljs-comment"># Check predictions before training</span>
<span class="hljs-keyword">with</span> torch.no_grad():
    precheck_sent = prepare_sequence(training_data[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>], word_to_ix)
    precheck_tags = torch.tensor([tag_to_ix[t] <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> training_data[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>]], dtype=torch.long)
    print(model(precheck_sent))

<span class="hljs-comment"># Make sure prepare_sequence from earlier in the LSTM section is loaded</span>
<span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(
        <span class="hljs-number">300</span>):  <span class="hljs-comment"># again, normally you would NOT do 300 epochs, it is toy data</span>
    <span class="hljs-keyword">for</span> sentence, tags <span class="hljs-keyword">in</span> training_data:
        <span class="hljs-comment"># Step 1\. Remember that Pytorch accumulates gradients.</span>
        <span class="hljs-comment"># We need to clear them out before each instance</span>
        model.zero_grad()

        <span class="hljs-comment"># Step 2\. Get our inputs ready for the network, that is,</span>
        <span class="hljs-comment"># turn them into Tensors of word indices.</span>
        sentence_in = prepare_sequence(sentence, word_to_ix)
        targets = torch.tensor([tag_to_ix[t] <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> tags], dtype=torch.long)

        <span class="hljs-comment"># Step 3\. Run our forward pass.</span>
        loss = model.neg_log_likelihood(sentence_in, targets)

        <span class="hljs-comment"># Step 4\. Compute the loss, gradients, and update the parameters by</span>
        <span class="hljs-comment"># calling optimizer.step()</span>
        loss.backward()
        optimizer.step()

<span class="hljs-comment"># Check predictions after training</span>
<span class="hljs-keyword">with</span> torch.no_grad():
    precheck_sent = prepare_sequence(training_data[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>], word_to_ix)
    print(model(precheck_sent))
<span class="hljs-comment"># We got it!</span>
</code></pre>
<p>Out:</p>
<pre><code class="lang-py">(tensor(<span class="hljs-number">2.6907</span>), [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>])
(tensor(<span class="hljs-number">20.4906</span>), [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>])
</code></pre>
<h2 id="exercise-a-new-loss-function-for-discriminative-tagging">Exercise: A new loss function for discriminative tagging</h2>
<p>It wasn&#x2019;t really necessary for us to create a computation graph when doing decoding, since we do not backpropagate from the viterbi path score. Since we have it anyway, try training the tagger where the loss function is the difference between the Viterbi path score and the score of the gold-standard path. It should be clear that this function is non-negative and 0 when the predicted tag sequence is the correct tag sequence. This is essentially <em>structured perceptron</em>.</p>
<p>This modification should be short, since Viterbi and score<em>sentence are already implemented. This is an example of the shape of the computation graph _depending on the training instance</em>. Although I haven&#x2019;t tried implementing this in a static toolkit, I imagine that it is possible but much less straightforward.</p>
<p>Pick up some real data and do a comparison!</p>
<footer class="page-footer"><span class="copyright">Copyright &#xA9; ibooker.org.cn 2019 all right reserved&#xFF0C;&#x7531; ApacheCN &#x56E2;&#x961F;&#x63D0;&#x4F9B;&#x652F;&#x6301;</span><span class="footer-modification">&#x8BE5;&#x6587;&#x4EF6;&#x4FEE;&#x8BA2;&#x65F6;&#x95F4;&#xFF1A; 
2019-06-17 07:36:30
</span></footer>
<script>console.log("plugin-popup....");document.onclick = function(e){ e.target.tagName === "IMG" && window.open(e.target.src,e.target.src)}</script><style>img{cursor:pointer}</style>
                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="nlp_sequence_models_tutorial.html" class="navigation navigation-prev " aria-label="Previous page: 序列模型和 LSTM 网络">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="seq2seq_translation_tutorial.html" class="navigation navigation-next " aria-label="Next page: 基于注意力机制的 seq2seq 神经网络翻译">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"Advanced: Making Dynamic Decisions and the Bi-LSTM CRF","level":"1.2.3.4.5","depth":4,"next":{"title":"基于注意力机制的 seq2seq 神经网络翻译","level":"1.2.3.5","depth":3,"path":"seq2seq_translation_tutorial.md","ref":"seq2seq_translation_tutorial.md","articles":[]},"previous":{"title":"序列模型和 LSTM 网络","level":"1.2.3.4.4","depth":4,"path":"nlp_sequence_models_tutorial.md","ref":"nlp_sequence_models_tutorial.md","articles":[]},"dir":"ltr"},"config":{"plugins":["github","-sharing","insert-logo","sharing-plus","back-to-top-button","code","copy-code-button","mathjax","pageview-count","edit-link","emphasize","alerts","auto-scroll-table","popup","hide-element","page-toc-button","github-buttons","tbfed-pagefooter"],"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"pluginsConfig":{"tbfed-pagefooter":{"copyright":"Copyright &copy ibooker.org.cn 2019","modify_label":"该文件修订时间： ","modify_format":"YYYY-MM-DD HH:mm:ss"},"emphasize":{},"github":{"url":"https://github.com/apachecn"},"search":{},"sharing-plus":{"qq":false,"all":["facebook","google","twitter","instapaper","linkedin","pocket","stumbleupon"],"douban":false,"facebook":true,"weibo":false,"instapaper":false,"whatsapp":false,"hatenaBookmark":false,"twitter":true,"messenger":false,"line":false,"vk":false,"pocket":true,"google":false,"viber":false,"stumbleupon":false,"qzone":false,"linkedin":false},"auto-scroll-table":{},"popup":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"code":{"copyButtons":true},"hide-element":{"elements":[".gitbook-link"]},"fontsettings":{"theme":"white","family":"sans","size":2},"highlight":{},"page-toc-button":{"maxTocDepth":4,"minTocSize":4},"back-to-top-button":{},"pageview-count":{},"alerts":{},"github-buttons":{},"mathjax":{"forceSVG":false,"version":"2.6-latest"},"copy-code-button":{},"sharing":{"qq":false,"all":["qq","douban","facebook","google","linkedin","twitter","weibo","whatsapp"],"douban":false,"facebook":false,"weibo":true,"whatsapp":false,"twitter":false,"line":false,"google":false,"qzone":true},"edit-link":{"label":"编辑本页","base":"https://github.com/apachecn/pytorch-doc-zh/blob/master"},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false},"insert-logo":{"style":"background: none; max-height: 150px; min-height: 150px","url":"http://data.apachecn.org/img/logo.jpg"}},"my_links":{"sidebar":{"Home":"https://www.baidu.com"}},"theme":"default","author":"ApacheCN","my_plugins":["splitter","donate","search-pro","todo","-lunr","-search","expandable-chapters-small","chapter-fold","expandable-chapters","expandable-chapters-small","back-to-top-button","advanced-emoji","tbfed-pagefooter","sectionx","page-treeview","simple-page-toc","github-buttons","ancre-navigation"],"my_pluginsConfig":{"ignores":["node_modules"],"simple-page-toc":{"maxDepth":3,"skipFirstH1":true},"page-toc-button":{"maxTocDepth":2,"minTocSize":2},"page-treeview":{"copyright":"Copyright &#169; aleen42","minHeaderCount":"2","minHeaderDeep":"2"},"donate":{"wechat":"微信收款的二维码URL","alipay":"支付宝收款的二维码URL","title":"","button":"赏","alipayText":"支付宝打赏","wechatText":"微信打赏"},"page-copyright":{"wisdom":"Designer, Frontend Developer & overall web enthusiast","noPowered":false,"copyright":"Copyright &#169; 你的名字","style":"normal","timeColor":"#666","utcOffset":"8","format":"YYYY-MM-dd hh:mm:ss","signature":"你的签名","copyrightColor":"#666","description":"modified at"}},"pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"variables":{},"title":"Pytorch 中文文档","language":"zh-hans","gitbook":"*","description":"Pytorch 中文文档: 教程和文档"},"file":{"path":"nlp_advanced_tutorial.md","mtime":"2019-06-17T07:36:30.744Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2019-06-17T07:38:27.820Z"},"basePath":".","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="gitbook/gitbook.js"></script>
    <script src="gitbook/theme.js"></script>
    
        
        <script src="gitbook/gitbook-plugin-github/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-insert-logo/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-sharing-plus/buttons.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-back-to-top-button/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-code/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-copy-code-button/toggle.js"></script>
        
    
        
        <script src="https://cdn.mathjax.org/mathjax/2.6-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-mathjax/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-pageview-count/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-edit-link/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-alerts/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-auto-scroll-table/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-hide-element/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-page-toc-button/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-github-buttons/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

