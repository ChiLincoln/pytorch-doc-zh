
<!DOCTYPE HTML>
<html lang="zh-hans" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>Extending TorchScript with Custom C++   Operators · Pytorch 中文文档</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.3">
        <meta name="author" content="ApacheCN">
        
        
    
    <link rel="stylesheet" href="gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-insert-logo/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-back-to-top-button/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-code/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-pageview-count/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-emphasize/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-alerts/style.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-auto-scroll-table/website.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-page-toc-button/plugin.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-tbfed-pagefooter/footer.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-search/search.css">
                
            
                
                <link rel="stylesheet" href="gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="tut_production_usage.html" />
    
    
    <link rel="prev" href="cpp_extension.html" />
    

    <style>
    @media only screen and (max-width: 640px) {
        .book-header .hidden-mobile {
            display: none;
        }
    }
    </style>
    <script>
        window["gitbook-plugin-github-buttons"] = {};
    </script>

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
<div id="book-search-input" role="search">
    <input type="text" placeholder="输入并搜索" />
</div>

            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="./">
            
                <a href="./">
            
                    
                    Introduction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" >
            
                <span>
            
                    
                    中文教程
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.1" data-path="tut_getting_started.html">
            
                <a href="tut_getting_started.html">
            
                    
                    起步
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.1.1" data-path="deep_learning_60min_blitz.html">
            
                <a href="deep_learning_60min_blitz.html">
            
                    
                    PyTorch 深度学习: 60 分钟极速入门
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.1.1.1" data-path="blitz_tensor_tutorial.html">
            
                <a href="blitz_tensor_tutorial.html">
            
                    
                    什么是 PyTorch？
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.1.2" data-path="blitz_autograd_tutorial.html">
            
                <a href="blitz_autograd_tutorial.html">
            
                    
                    Autograd：自动求导
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.1.3" data-path="blitz_neural_networks_tutorial.html">
            
                <a href="blitz_neural_networks_tutorial.html">
            
                    
                    神经网络
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.1.4" data-path="blitz_cifar10_tutorial.html">
            
                <a href="blitz_cifar10_tutorial.html">
            
                    
                    训练分类器
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.1.5" data-path="blitz_data_parallel_tutorial.html">
            
                <a href="blitz_data_parallel_tutorial.html">
            
                    
                    可选：数据并行处理
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.1.2" data-path="data_loading_tutorial.html">
            
                <a href="data_loading_tutorial.html">
            
                    
                    数据加载和处理教程
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.3" data-path="pytorch_with_examples.html">
            
                <a href="pytorch_with_examples.html">
            
                    
                    用例子学习 PyTorch
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.4" data-path="transfer_learning_tutorial.html">
            
                <a href="transfer_learning_tutorial.html">
            
                    
                    迁移学习教程
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.5" data-path="deploy_seq2seq_hybrid_frontend_tutorial.html">
            
                <a href="deploy_seq2seq_hybrid_frontend_tutorial.html">
            
                    
                    混合前端的 seq2seq 模型部署
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.6" data-path="saving_loading_models.html">
            
                <a href="saving_loading_models.html">
            
                    
                    Saving and Loading Models
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.1.7" data-path="nn_tutorial.html">
            
                <a href="nn_tutorial.html">
            
                    
                    What is torch.nn really?
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.2" data-path="tut_image.html">
            
                <a href="tut_image.html">
            
                    
                    图像
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.2.1" data-path="finetuning_torchvision_models_tutorial.html">
            
                <a href="finetuning_torchvision_models_tutorial.html">
            
                    
                    Torchvision 模型微调
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2.2" data-path="spatial_transformer_tutorial.html">
            
                <a href="spatial_transformer_tutorial.html">
            
                    
                    空间变换器网络教程
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2.3" data-path="neural_style_tutorial.html">
            
                <a href="neural_style_tutorial.html">
            
                    
                    使用 PyTorch 进行图像风格转换
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2.4" data-path="fgsm_tutorial.html">
            
                <a href="fgsm_tutorial.html">
            
                    
                    对抗性示例生成
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2.5" data-path="super_resolution_with_caffe2.html">
            
                <a href="super_resolution_with_caffe2.html">
            
                    
                    使用 ONNX 将模型从 PyTorch 传输到 Caffe2 和移动端
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.3" data-path="tut_text.html">
            
                <a href="tut_text.html">
            
                    
                    文本
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.3.1" data-path="chatbot_tutorial.html">
            
                <a href="chatbot_tutorial.html">
            
                    
                    聊天机器人教程
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.3.2" data-path="char_rnn_generation_tutorial.html">
            
                <a href="char_rnn_generation_tutorial.html">
            
                    
                    使用字符级别特征的 RNN 网络生成姓氏
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.3.3" data-path="char_rnn_classification_tutorial.html">
            
                <a href="char_rnn_classification_tutorial.html">
            
                    
                    使用字符级别特征的 RNN 网络进行姓氏分类
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.3.4" data-path="deep_learning_nlp_tutorial.html">
            
                <a href="deep_learning_nlp_tutorial.html">
            
                    
                    Deep Learning for NLP with Pytorch
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.3.4.1" data-path="nlp_pytorch_tutorial.html">
            
                <a href="nlp_pytorch_tutorial.html">
            
                    
                    PyTorch 介绍
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.3.4.2" data-path="nlp_deep_learning_tutorial.html">
            
                <a href="nlp_deep_learning_tutorial.html">
            
                    
                    使用 PyTorch 进行深度学习
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.3.4.3" data-path="nlp_word_embeddings_tutorial.html">
            
                <a href="nlp_word_embeddings_tutorial.html">
            
                    
                    Word Embeddings: Encoding Lexical Semantics
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.3.4.4" data-path="nlp_sequence_models_tutorial.html">
            
                <a href="nlp_sequence_models_tutorial.html">
            
                    
                    序列模型和 LSTM 网络
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.3.4.5" data-path="nlp_advanced_tutorial.html">
            
                <a href="nlp_advanced_tutorial.html">
            
                    
                    Advanced: Making Dynamic Decisions and the Bi-LSTM CRF
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.3.5" data-path="seq2seq_translation_tutorial.html">
            
                <a href="seq2seq_translation_tutorial.html">
            
                    
                    基于注意力机制的 seq2seq 神经网络翻译
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.4" data-path="tut_generative.html">
            
                <a href="tut_generative.html">
            
                    
                    生成
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.4.1" data-path="dcgan_faces_tutorial.html">
            
                <a href="dcgan_faces_tutorial.html">
            
                    
                    DCGAN Tutorial
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.5" data-path="tut_reinforcement_learning.html">
            
                <a href="tut_reinforcement_learning.html">
            
                    
                    强化学习
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.5.1" data-path="reinforcement_q_learning.html">
            
                <a href="reinforcement_q_learning.html">
            
                    
                    Reinforcement Learning (DQN) Tutorial
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.6" data-path="tut_extending_pytorch.html">
            
                <a href="tut_extending_pytorch.html">
            
                    
                    扩展 PyTorch
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.6.1" data-path="numpy_extensions_tutorial.html">
            
                <a href="numpy_extensions_tutorial.html">
            
                    
                    用 numpy 和 scipy 创建扩展
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.6.2" data-path="cpp_extension.html">
            
                <a href="cpp_extension.html">
            
                    
                    Custom C++   and CUDA Extensions
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="1.2.6.3" data-path="torch_script_custom_ops.html">
            
                <a href="torch_script_custom_ops.html">
            
                    
                    Extending TorchScript with Custom C++   Operators
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.7" data-path="tut_production_usage.html">
            
                <a href="tut_production_usage.html">
            
                    
                    生产性使用
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.7.1" data-path="dist_tuto.html">
            
                <a href="dist_tuto.html">
            
                    
                    Writing Distributed Applications with PyTorch
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7.2" data-path="aws_distributed_training_tutorial.html">
            
                <a href="aws_distributed_training_tutorial.html">
            
                    
                    使用 Amazon AWS 进行分布式训练
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7.3" data-path="ONNXLive.html">
            
                <a href="ONNXLive.html">
            
                    
                    ONNX 现场演示教程
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.7.4" data-path="cpp_export.html">
            
                <a href="cpp_export.html">
            
                    
                    在 C++ 中加载 PYTORCH 模型
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.2.8" data-path="tut_other_language.html">
            
                <a href="tut_other_language.html">
            
                    
                    其它语言中的 PyTorch
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.8.1" data-path="cpp_frontend.html">
            
                <a href="cpp_frontend.html">
            
                    
                    使用 PyTorch C++ 前端
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.3" >
            
                <span>
            
                    
                    中文文档
            
                </span>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.1" data-path="docs_notes.html">
            
                <a href="docs_notes.html">
            
                    
                    注解
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.1.1" data-path="notes_autograd.html">
            
                <a href="notes_autograd.html">
            
                    
                    自动求导机制
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.2" data-path="notes_broadcasting.html">
            
                <a href="notes_broadcasting.html">
            
                    
                    广播语义
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.3" data-path="notes_cuda.html">
            
                <a href="notes_cuda.html">
            
                    
                    CUDA 语义
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.4" data-path="notes_extending.html">
            
                <a href="notes_extending.html">
            
                    
                    Extending PyTorch
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.5" data-path="notes_faq.html">
            
                <a href="notes_faq.html">
            
                    
                    Frequently Asked Questions
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.6" data-path="notes_multiprocessing.html">
            
                <a href="notes_multiprocessing.html">
            
                    
                    Multiprocessing best practices
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.7" data-path="notes_randomness.html">
            
                <a href="notes_randomness.html">
            
                    
                    Reproducibility
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.8" data-path="notes_serialization.html">
            
                <a href="notes_serialization.html">
            
                    
                    Serialization semantics
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.1.9" data-path="notes_windows.html">
            
                <a href="notes_windows.html">
            
                    
                    Windows FAQ
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.3.2" data-path="docs_package_ref.html">
            
                <a href="docs_package_ref.html">
            
                    
                    包参考
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.2.1" data-path="torch.html">
            
                <a href="torch.html">
            
                    
                    torch
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.2" data-path="tensors.html">
            
                <a href="tensors.html">
            
                    
                    torch.Tensor
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.3" data-path="tensor_attributes.html">
            
                <a href="tensor_attributes.html">
            
                    
                    Tensor Attributes
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.4" data-path="type_info.html">
            
                <a href="type_info.html">
            
                    
                    数据类型信息
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.5" data-path="sparse.html">
            
                <a href="sparse.html">
            
                    
                    torch.sparse
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.6" data-path="cuda.html">
            
                <a href="cuda.html">
            
                    
                    torch.cuda
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.7" data-path="storage.html">
            
                <a href="storage.html">
            
                    
                    torch.Storage
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.8" data-path="nn.html">
            
                <a href="nn.html">
            
                    
                    torch.nn
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.9" data-path="nn_functional.html">
            
                <a href="nn_functional.html">
            
                    
                    torch.nn.functional
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.10" data-path="nn_init.html">
            
                <a href="nn_init.html">
            
                    
                    torch.nn.init
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.11" data-path="optim.html">
            
                <a href="optim.html">
            
                    
                    torch.optim
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.12" data-path="autograd.html">
            
                <a href="autograd.html">
            
                    
                    Automatic differentiation package - torch.autograd
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.13" data-path="distributed.html">
            
                <a href="distributed.html">
            
                    
                    Distributed communication package - torch.distributed
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.14" data-path="distributions.html">
            
                <a href="distributions.html">
            
                    
                    Probability distributions - torch.distributions
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.15" data-path="jit.html">
            
                <a href="jit.html">
            
                    
                    Torch Script
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.16" data-path="multiprocessing.html">
            
                <a href="multiprocessing.html">
            
                    
                    多进程包 - torch.multiprocessing
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.17" data-path="bottleneck.html">
            
                <a href="bottleneck.html">
            
                    
                    torch.utils.bottleneck
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.18" data-path="checkpoint.html">
            
                <a href="checkpoint.html">
            
                    
                    torch.utils.checkpoint
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.19" data-path="docs_cpp_extension.html">
            
                <a href="docs_cpp_extension.html">
            
                    
                    torch.utils.cpp_extension
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.20" data-path="data.html">
            
                <a href="data.html">
            
                    
                    torch.utils.data
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.21" data-path="dlpack.html">
            
                <a href="dlpack.html">
            
                    
                    torch.utils.dlpack
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.22" data-path="hub.html">
            
                <a href="hub.html">
            
                    
                    torch.hub
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.23" data-path="model_zoo.html">
            
                <a href="model_zoo.html">
            
                    
                    torch.utils.model_zoo
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.24" data-path="onnx.html">
            
                <a href="onnx.html">
            
                    
                    torch.onnx
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.2.25" data-path="distributed_deprecated.html">
            
                <a href="distributed_deprecated.html">
            
                    
                    Distributed communication package (deprecated) - torch.distributed.deprecated
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.3.3" data-path="docs_torchvision_ref.html">
            
                <a href="docs_torchvision_ref.html">
            
                    
                    torchvision 参考
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.3.3.1" data-path="torchvision_datasets.html">
            
                <a href="torchvision_datasets.html">
            
                    
                    torchvision.datasets
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.2" data-path="torchvision_models.html">
            
                <a href="torchvision_models.html">
            
                    
                    torchvision.models
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.3" data-path="torchvision_transforms.html">
            
                <a href="torchvision_transforms.html">
            
                    
                    torchvision.transforms
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.3.3.4" data-path="torchvision_utils.html">
            
                <a href="torchvision_utils.html">
            
                    
                    torchvision.utils
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

            </ul>
            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            本书使用 GitBook 发布
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href="." >Extending TorchScript with Custom C++   Operators</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
<div id="book-search-results">
    <div class="search-noresults">
    
                                <section class="normal markdown-section">
                                
                                <h1 id="extending-torchscript-with-custom-c-operators">Extending TorchScript with Custom C++ Operators</h1>
<p>The PyTorch 1.0 release introduced a new programming model to PyTorch called <a href="https://pytorch.org/docs/master/jit.html" target="_blank">TorchScript</a>. TorchScript is a subset of the Python programming language which can be parsed, compiled and optimized by the TorchScript compiler. Further, compiled TorchScript models have the option of being serialized into an on-disk file format, which you can subsequently load and run from pure C++ (as well as Python) for inference.</p>
<p>TorchScript supports a large subset of operations provided by the <code>torch</code> package, allowing you to express many kinds of complex models purely as a series of tensor operations from PyTorch&#x2019;s &#x201C;standard library&#x201D;. Nevertheless, there may be times where you find yourself in need of extending TorchScript with a custom C++ or CUDA function. While we recommend that you only resort to this option if your idea cannot be expressed (efficiently enough) as a simple Python function, we do provide a very friendly and simple interface for defining custom C++ and CUDA kernels using <a href="https://pytorch.org/cppdocs/#aten" target="_blank">ATen</a>, PyTorch&#x2019;s high performance C++ tensor library. Once bound into TorchScript, you can embed these custom kernels (or &#x201C;ops&#x201D;) into your TorchScript model and execute them both in Python and in their serialized form directly in C++.</p>
<p>The following paragraphs give an example of writing a TorchScript custom op to call into <a href="https://www.opencv.org" target="_blank">OpenCV</a>, a computer vision library written in C++. We will discuss how to work with tensors in C++, how to efficiently convert them to third party tensor formats (in this case, OpenCV <a href="#id1">``</a>Mat``s), how to register your operator with the TorchScript runtime and finally how to compile the operator and use it in Python and C++.</p>
<p>This tutorial assumes you have the <em>preview release</em> of PyTorch 1.0 installed via <code>pip</code> or <code>conda</code>. See <a href="https://pytorch.org/get-started/locally" target="_blank">https://pytorch.org/get-started/locally</a> for instructions on grabbing the latest release of PyTorch 1.0. Alternatively, you can compile PyTorch from source. The documentation in <a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md" target="_blank">this file</a> will assist you with this.</p>
<h2 id="implementing-the-custom-operator-in-c">Implementing the Custom Operator in C++</h2>
<p>For this tutorial, we&#x2019;ll be exposing the <a href="https://docs.opencv.org/2.4/modules/imgproc/doc/geometric_transformations.html#warpperspective" target="_blank">warpPerspective</a> function, which applies a perspective transformation to an image, from OpenCV to TorchScript as a custom operator. The first step is to write the implementation of our custom operator in C++. Let&#x2019;s call the file for this implementation <code>op.cpp</code> and make it look like this:</p>
<pre><code class="lang-py"><span class="hljs-comment">#include &lt;opencv2/opencv.hpp&gt;</span>
<span class="hljs-comment">#include &lt;torch/script.h&gt;</span>

torch::Tensor warp_perspective(torch::Tensor image, torch::Tensor warp) {
  cv::Mat image_mat(/*rows=*/image.size(<span class="hljs-number">0</span>),
                    /*cols=*/image.size(<span class="hljs-number">1</span>),
                    /*type=*/CV_32FC1,
                    /*data=*/image.data&lt;float&gt;());
  cv::Mat warp_mat(/*rows=*/warp.size(<span class="hljs-number">0</span>),
                   /*cols=*/warp.size(<span class="hljs-number">1</span>),
                   /*type=*/CV_32FC1,
                   /*data=*/warp.data&lt;float&gt;());

  cv::Mat output_mat;
  cv::warpPerspective(image_mat, output_mat, warp_mat, /*dsize=*/{<span class="hljs-number">8</span>, <span class="hljs-number">8</span>});

  torch::Tensor output = torch::from_blob(output_mat.ptr&lt;float&gt;(), /*sizes=*/{<span class="hljs-number">8</span>, <span class="hljs-number">8</span>});
  <span class="hljs-keyword">return</span> output.clone();
}
</code></pre>
<p>The code for this operator is quite short. At the top of the file, we include the OpenCV header file, <code>opencv2/opencv.hpp</code>, alongside the <code>torch/script.h</code> header which exposes all the necessary goodies from PyTorch&#x2019;s C++ API that we need to write custom TorchScript operators. Our function <code>warp_perspective</code> takes two arguments: an input <code>image</code> and the <code>warp</code> transformation matrix we wish to apply to the image. The type of these inputs is <code>torch::Tensor</code>, PyTorch&#x2019;s tensor type in C++ (which is also the underlying type of all tensors in Python). The return type of our <code>warp_perspective</code> function will also be a <code>torch::Tensor</code>.</p>
<p>Tip</p>
<p>See <a href="https://pytorch.org/cppdocs/notes/tensor_basics.html" target="_blank">this note</a> for more information about ATen, the library that provides the <code>Tensor</code> class to PyTorch. Further, <a href="https://pytorch.org/cppdocs/notes/tensor_creation.html" target="_blank">this tutorial</a> describes how to allocate and initialize new tensor objects in C++ (not required for this operator).</p>
<p>Attention</p>
<p>The TorchScript compiler understands a fixed number of types. Only these types can be used as arguments to your custom operator. Currently these types are: <code>torch::Tensor</code>, <code>torch::Scalar</code>, <code>double</code>, <code>int64_t</code> and <code>std::vector``s of these types. Note that __only__ ``double</code> and <strong>not</strong> <code>float</code>, and <strong>only</strong> <code>int64_t</code> and <strong>not</strong> other integral types such as <code>int</code>, <code>short</code> or <code>long</code> are supported.</p>
<p>Inside of our function, the first thing we need to do is convert our PyTorch tensors to OpenCV matrices, as OpenCV&#x2019;s <code>warpPerspective</code> expects <code>cv::Mat</code> objects as inputs. Fortunately, there is a way to do this <strong>without copying any</strong> data. In the first few lines,</p>
<pre><code class="lang-py">cv::Mat image_mat(/*rows=*/image.size(<span class="hljs-number">0</span>),
                  /*cols=*/image.size(<span class="hljs-number">1</span>),
                  /*type=*/CV_32FC1,
                  /*data=*/image.data&lt;float&gt;());
</code></pre>
<p>we are calling <a href="https://docs.opencv.org/trunk/d3/d63/classcv_1_1Mat.html#a922de793eabcec705b3579c5f95a643e" target="_blank">this constructor</a> of the OpenCV <code>Mat</code> class to convert our tensor to a <code>Mat</code> object. We pass it the number of rows and columns of the original <code>image</code> tensor, the datatype (which we&#x2019;ll fix as <code>float32</code> for this example), and finally a raw pointer to the underlying data &#x2013; a <code>float*</code>. What is special about this constructor of the <code>Mat</code> class is that it does not copy the input data. Instead, it will simply reference this memory for all operations performed on the <code>Mat</code>. If an in-place operation is performed on the <code>image_mat</code>, this will be reflected in the original <code>image</code> tensor (and vice-versa). This allows us to call subsequent OpenCV routines with the library&#x2019;s native matrix type, even though we&#x2019;re actually storing the data in a PyTorch tensor. We repeat this procedure to convert the <code>warp</code> PyTorch tensor to the <code>warp_mat</code> OpenCV matrix:</p>
<pre><code class="lang-py">cv::Mat warp_mat(/*rows=*/warp.size(<span class="hljs-number">0</span>),
                 /*cols=*/warp.size(<span class="hljs-number">1</span>),
                 /*type=*/CV_32FC1,
                 /*data=*/warp.data&lt;float&gt;());
</code></pre>
<p>Next, we are ready to call the OpenCV function we were so eager to use in TorchScript: <code>warpPerspective</code>. For this, we pass the OpenCV function the <code>image_mat</code> and <code>warp_mat</code> matrices, as well as an empty output matrix called <code>output_mat</code>. We also specify the size <code>dsize</code> we want the output matrix (image) to be. It is hardcoded to <code>8 x 8</code> for this example:</p>
<pre><code class="lang-py">cv::Mat output_mat;
cv::warpPerspective(image_mat, output_mat, warp_mat, /*dsize=*/{<span class="hljs-number">8</span>, <span class="hljs-number">8</span>});
</code></pre>
<p>The final step in our custom operator implementation is to convert the <code>output_mat</code> back into a PyTorch tensor, so that we can further use it in PyTorch. This is strikingly similar to what we did earlier to convert in the other direction. In this case, PyTorch provides a <code>torch::from_blob</code> method. A <em>blob</em> in this case is intended to mean some opaque, flat pointer to memory that we want to interpret as a PyTorch tensor. The call to <code>torch::from_blob</code> looks like this:</p>
<pre><code class="lang-py">torch::from_blob(output_mat.ptr&lt;float&gt;(), /*sizes=*/{<span class="hljs-number">8</span>, <span class="hljs-number">8</span>})
</code></pre>
<p>We use the <code>.ptr&amp;lt;float&amp;gt;()</code> method on the OpenCV <code>Mat</code> class to get a raw pointer to the underlying data (just like <code>.data&amp;lt;float&amp;gt;()</code> for the PyTorch tensor earlier). We also specify the output shape of the tensor, which we hardcoded as <code>8 x 8</code>. The output of <code>torch::from_blob</code> is then a <code>torch::Tensor</code>, pointing to the memory owned by the OpenCV matrix.</p>
<p>Before returning this tensor from our operator implementation, we must call <code>.clone()</code> on the tensor to perform a memory copy of the underlying data. The reason for this is that <code>torch::from_blob</code> returns a tensor that does not own its data. At that point, the data is still owned by the OpenCV matrix. However, this OpenCV matrix will go out of scope and be deallocated at the end of the function. If we returned the <code>output</code> tensor as-is, it would point to invalid memory by the time we use it outside the function. Calling <code>.clone()</code> returns a new tensor with a copy of the original data that the new tensor owns itself. It is thus safe to return to the outside world.</p>
<h2 id="registering-the-custom-operator-with-torchscript">Registering the Custom Operator with TorchScript</h2>
<p>Now that have implemented our custom operator in C++, we need to <em>register</em> it with the TorchScript runtime and compiler. This will allow the TorchScript compiler to resolve references to our custom operator in TorchScript code. Registration is very simple. For our case, we need to write:</p>
<pre><code class="lang-py">static auto registry =
  torch::jit::RegisterOperators(<span class="hljs-string">&quot;my_ops::warp_perspective&quot;</span>, &amp;warp_perspective);
</code></pre>
<p>somewhere in the global scope of our <code>op.cpp</code> file. This creates a global variable <code>registry</code>, which will register our operator with TorchScript in its constructor (i.e. exactly once per program). We specify the name of the operator, and a pointer to its implementation (the function we wrote earlier). The name consists of two parts: a <em>namespace</em> (<code>my_ops</code>) and a name for the particular operator we are registering (<code>warp_perspective</code>). The namespace and operator name are separated by two colons (<code>::</code>).</p>
<p>Tip</p>
<p>If you want to register more than one operator, you can chain calls to <code>.op()</code> after the constructor:</p>
<pre><code class="lang-py">static auto registry =
  torch::jit::RegisterOperators(<span class="hljs-string">&quot;my_ops::warp_perspective&quot;</span>, &amp;warp_perspective)
  .op(<span class="hljs-string">&quot;my_ops::another_op&quot;</span>, &amp;another_op)
  .op(<span class="hljs-string">&quot;my_ops::and_another_op&quot;</span>, &amp;and_another_op);
</code></pre>
<p>Behind the scenes, <code>RegisterOperators</code> will perform a number of fairly complicated C++ template metaprogramming magic tricks to infer the argument and return value types of the function pointer we pass it (<code>&amp;warp_perspective</code>). This information is used to form a <em>function schema</em> for our operator. A function schema is a structured representation of an operator &#x2013; a kind of &#x201C;signature&#x201D; or &#x201C;prototype&#x201D; &#x2013; used by the TorchScript compiler to verify correctness in TorchScript programs.</p>
<h2 id="building-the-custom-operator">Building the Custom Operator</h2>
<p>Now that we have implemented our custom operator in C++ and written its registration code, it is time to build the operator into a (shared) library that we can load into Python for research and experimentation, or into C++ for inference in a no-Python environment. There exist multiple ways to build our operator, using either pure CMake, or Python alternatives like <code>setuptools</code>. For brevity, the paragraphs below only discuss the CMake approach. The appendix of this tutorial dives into the Python based alternatives.</p>
<h3 id="building-with-cmake">Building with CMake</h3>
<p>To build our custom operator into a shared library using the <a href="https://cmake.org" target="_blank">CMake</a> build system, we need to write a short <code>CMakeLists.txt</code> file and place it with our previous <code>op.cpp</code> file. For this, let&#x2019;s agree on a a directory structure that looks like this:</p>
<pre><code class="lang-py">warp-perspective/
  op.cpp
  CMakeLists.txt
</code></pre>
<p>Also, make sure to grab the latest version of the LibTorch distribution, which packages PyTorch&#x2019;s C++ libraries and CMake build files, from <a href="https://pytorch.org/get-started/locally" target="_blank">pytorch.org</a>. Place the unzipped distribution somewhere accessible in your file system. The following paragraphs will refer to that location as <code>/path/to/libtorch</code>. The contents of our <code>CMakeLists.txt</code> file should then be the following:</p>
<pre><code class="lang-py">cmake_minimum_required(VERSION <span class="hljs-number">3.1</span> FATAL_ERROR)
project(warp_perspective)

find_package(Torch REQUIRED)
find_package(OpenCV REQUIRED)

<span class="hljs-comment"># Define our library target</span>
add_library(warp_perspective SHARED op.cpp)
<span class="hljs-comment"># Enable C++11</span>
target_compile_features(warp_perspective PRIVATE cxx_range_for)
<span class="hljs-comment"># Link against LibTorch</span>
target_link_libraries(warp_perspective <span class="hljs-string">&quot;${TORCH_LIBRARIES}&quot;</span>)
<span class="hljs-comment"># Link against OpenCV</span>
target_link_libraries(warp_perspective opencv_core opencv_imgproc)
</code></pre>
<p>Warning</p>
<p>This setup makes some assumptions about the build environment, particularly what pertains to the installation of OpenCV. The above <code>CMakeLists.txt</code> file was tested inside a Docker container running Ubuntu Xenial with <code>libopencv-dev</code> installed via <code>apt</code>. If it does not work for you and you feel stuck, please use the <code>Dockerfile</code> in the <a href="https://github.com/pytorch/extension-script" target="_blank">accompanying tutorial repository</a> to build an isolated, reproducible environment in which to play around with the code from this tutorial. If you run into further troubles, please file an issue in the tutorial repository or post a question in <a href="https://discuss.pytorch.org/" target="_blank">our forum</a>.</p>
<p>To now build our operator, we can run the following commands from our <code>warp_perspective</code> folder:</p>
<pre><code class="lang-py">$ mkdir build
$ cd build
$ cmake -DCMAKE_PREFIX_PATH=/path/to/libtorch ..
-- The C compiler identification <span class="hljs-keyword">is</span> GNU <span class="hljs-number">5.4</span><span class="hljs-number">.0</span>
-- The CXX compiler identification <span class="hljs-keyword">is</span> GNU <span class="hljs-number">5.4</span><span class="hljs-number">.0</span>
-- Check <span class="hljs-keyword">for</span> working C compiler: /usr/bin/cc
-- Check <span class="hljs-keyword">for</span> working C compiler: /usr/bin/cc -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Detecting C compile features
-- Detecting C compile features - done
-- Check <span class="hljs-keyword">for</span> working CXX compiler: /usr/bin/c++
-- Check <span class="hljs-keyword">for</span> working CXX compiler: /usr/bin/c++ -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Looking <span class="hljs-keyword">for</span> pthread.h
-- Looking <span class="hljs-keyword">for</span> pthread.h - found
-- Looking <span class="hljs-keyword">for</span> pthread_create
-- Looking <span class="hljs-keyword">for</span> pthread_create - <span class="hljs-keyword">not</span> found
-- Looking <span class="hljs-keyword">for</span> pthread_create <span class="hljs-keyword">in</span> pthreads
-- Looking <span class="hljs-keyword">for</span> pthread_create <span class="hljs-keyword">in</span> pthreads - <span class="hljs-keyword">not</span> found
-- Looking <span class="hljs-keyword">for</span> pthread_create <span class="hljs-keyword">in</span> pthread
-- Looking <span class="hljs-keyword">for</span> pthread_create <span class="hljs-keyword">in</span> pthread - found
-- Found Threads: TRUE
-- Found torch: /libtorch/lib/libtorch.so
-- Configuring done
-- Generating done
-- Build files have been written to: /warp_perspective/build
$ make -j
Scanning dependencies of target warp_perspective
[ <span class="hljs-number">50</span>%] Building CXX object CMakeFiles/warp_perspective.dir/op.cpp.o
[<span class="hljs-number">100</span>%] Linking CXX shared library libwarp_perspective.so
[<span class="hljs-number">100</span>%] Built target warp_perspective
</code></pre>
<p>which will place a <code>libwarp_perspective.so</code> shared library file in the <code>build</code> folder. In the <code>cmake</code> command above, you should replace <code>/path/to/libtorch</code> with the path to your unzipped LibTorch distribution.</p>
<p>We will explore how to use and call our operator in detail further below, but to get an early sensation of success, we can try running the following code in Python:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span>torch.ops.load_library(<span class="hljs-string">&quot;/path/to/libwarp_perspective.so&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>print(torch.ops.my_ops.warp_perspective)
</code></pre>
<p>Here, <code>/path/to/libwarp_perspective.so</code> should be a relative or absolute path to the <code>libwarp_perspective.so</code> shared library we just built. If all goes well, this should print something like</p>
<pre><code class="lang-py">&lt;built-<span class="hljs-keyword">in</span> method my_ops::warp_perspective of PyCapsule object at <span class="hljs-number">0x7f618fc6fa50</span>&gt;
</code></pre>
<p>which is the Python function we will later use to invoke our custom operator.</p>
<h2 id="using-the-torchscript-custom-operator-in-python">Using the TorchScript Custom Operator in Python</h2>
<p>Once our custom operator is built into a shared library we are ready to use this operator in our TorchScript models in Python. There are two parts to this: first loading the operator into Python, and second using the operator in TorchScript code.</p>
<p>You already saw how to import your operator into Python: <code>torch.ops.load_library()</code>. This function takes the path to a shared library containing custom operators, and loads it into the current process. Loading the shared library will also execute the constructor of the global <code>RegisterOperators</code> object we placed into our custom operator implementation file. This will register our custom operator with the TorchScript compiler and allow us to use that operator in TorchScript code.</p>
<p>You can refer to your loaded operator as <code>torch.ops.&amp;lt;namespace&amp;gt;.&amp;lt;function&amp;gt;</code>, where <code>&amp;lt;namespace&amp;gt;</code> is the namespace part of your operator name, and <code>&amp;lt;function&amp;gt;</code> the function name of your operator. For the operator we wrote above, the namespace was <code>my_ops</code> and the function name <code>warp_perspective</code>, which means our operator is available as <code>torch.ops.my_ops.warp_perspective</code>. While this function can be used in scripted or traced TorchScript modules, we can also just use it in vanilla eager PyTorch and pass it regular PyTorch tensors:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span>torch.ops.load_library(<span class="hljs-string">&quot;libwarp_perspective.so&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>torch.ops.my_ops.warp_perspective(torch.randn(<span class="hljs-number">32</span>, <span class="hljs-number">32</span>), torch.rand(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>))
tensor([[<span class="hljs-number">0.0000</span>, <span class="hljs-number">0.3218</span>, <span class="hljs-number">0.4611</span>,  ..., <span class="hljs-number">0.4636</span>, <span class="hljs-number">0.4636</span>, <span class="hljs-number">0.4636</span>],
 [<span class="hljs-number">0.3746</span>, <span class="hljs-number">0.0978</span>, <span class="hljs-number">0.5005</span>,  ..., <span class="hljs-number">0.4636</span>, <span class="hljs-number">0.4636</span>, <span class="hljs-number">0.4636</span>],
 [<span class="hljs-number">0.3245</span>, <span class="hljs-number">0.0169</span>, <span class="hljs-number">0.0000</span>,  ..., <span class="hljs-number">0.4458</span>, <span class="hljs-number">0.4458</span>, <span class="hljs-number">0.4458</span>],
 ...,
 [<span class="hljs-number">0.1862</span>, <span class="hljs-number">0.1862</span>, <span class="hljs-number">0.1692</span>,  ..., <span class="hljs-number">0.0000</span>, <span class="hljs-number">0.0000</span>, <span class="hljs-number">0.0000</span>],
 [<span class="hljs-number">0.1862</span>, <span class="hljs-number">0.1862</span>, <span class="hljs-number">0.1692</span>,  ..., <span class="hljs-number">0.0000</span>, <span class="hljs-number">0.0000</span>, <span class="hljs-number">0.0000</span>],
 [<span class="hljs-number">0.1862</span>, <span class="hljs-number">0.1862</span>, <span class="hljs-number">0.1692</span>,  ..., <span class="hljs-number">0.0000</span>, <span class="hljs-number">0.0000</span>, <span class="hljs-number">0.0000</span>]])
</code></pre>
<p>Note</p>
<p>What happens behind the scenes is that the first time you access <code>torch.ops.namespace.function</code> in Python, the TorchScript compiler (in C++ land) will see if a function <code>namespace::function</code> has been registered, and if so, return a Python handle to this function that we can subsequently use to call into our C++ operator implementation from Python. This is one noteworthy difference between TorchScript custom operators and C++ extensions: C++ extensions are bound manually using pybind11, while TorchScript custom ops are bound on the fly by PyTorch itself. Pybind11 gives you more flexibility with regards to what types and classes you can bind into Python and is thus recommended for purely eager code, but it is not supported for TorchScript ops.</p>
<p>From here on, you can use your custom operator in scripted or traced code just as you would other functions from the <code>torch</code> package. In fact, &#x201C;standard library&#x201D; functions like <code>torch.matmul</code> go through largely the same registration path as custom operators, which makes custom operators really first-class citizens when it comes to how and where they can be used in TorchScript.</p>
<h3 id="using-the-custom-operator-with-tracing">Using the Custom Operator with Tracing</h3>
<p>Let&#x2019;s start by embedding our operator in a traced function. Recall that for tracing, we start with some vanilla Pytorch code:</p>
<pre><code class="lang-py"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">compute</span><span class="hljs-params">(x, y, z)</span>:</span>
    <span class="hljs-keyword">return</span> x.matmul(y) + torch.relu(z)
</code></pre>
<p>and then call <code>torch.jit.trace</code> on it. We further pass <code>torch.jit.trace</code> some example inputs, which it will forward to our implementation to record the sequence of operations that occur as the inputs flow through it. The result of this is effectively a &#x201C;frozen&#x201D; version of the eager PyTorch program, which the TorchScript compiler can further analyze, optimize and serialize:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>inputs = [torch.randn(<span class="hljs-number">4</span>, <span class="hljs-number">8</span>), torch.randn(<span class="hljs-number">8</span>, <span class="hljs-number">5</span>), torch.randn(<span class="hljs-number">4</span>, <span class="hljs-number">5</span>)]
<span class="hljs-meta">&gt;&gt;&gt; </span>trace = torch.jit.trace(compute, inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>print(trace.graph)
graph(%x : Float(<span class="hljs-number">4</span>, <span class="hljs-number">8</span>)
 %y : Float(<span class="hljs-number">8</span>, <span class="hljs-number">5</span>)
 %z : Float(<span class="hljs-number">4</span>, <span class="hljs-number">5</span>)) {
 %<span class="hljs-number">3</span> : Float(<span class="hljs-number">4</span>, <span class="hljs-number">5</span>) = aten::matmul(%x, %y)
 %<span class="hljs-number">4</span> : Float(<span class="hljs-number">4</span>, <span class="hljs-number">5</span>) = aten::relu(%z)
 %<span class="hljs-number">5</span> : int = prim::Constant[value=<span class="hljs-number">1</span>]()
 %<span class="hljs-number">6</span> : Float(<span class="hljs-number">4</span>, <span class="hljs-number">5</span>) = aten::add(%<span class="hljs-number">3</span>, %<span class="hljs-number">4</span>, %<span class="hljs-number">5</span>)
 <span class="hljs-keyword">return</span> (%<span class="hljs-number">6</span>);
}
</code></pre>
<p>Now, the exciting revelation is that we can simply drop our custom operator into our PyTorch trace as if it were <code>torch.relu</code> or any other <code>torch</code> function:</p>
<pre><code class="lang-py">torch.ops.load_library(<span class="hljs-string">&quot;libwarp_perspective.so&quot;</span>)

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">compute</span><span class="hljs-params">(x, y, z)</span>:</span>
    x = torch.ops.my_ops.warp_perspective(x, torch.eye(<span class="hljs-number">3</span>))
    <span class="hljs-keyword">return</span> x.matmul(y) + torch.relu(z)
</code></pre>
<p>and then trace it as before:</p>
<pre><code class="lang-py"><span class="hljs-meta">&gt;&gt;&gt; </span>inputs = [torch.randn(<span class="hljs-number">4</span>, <span class="hljs-number">8</span>), torch.randn(<span class="hljs-number">8</span>, <span class="hljs-number">5</span>), torch.randn(<span class="hljs-number">8</span>, <span class="hljs-number">5</span>)]
<span class="hljs-meta">&gt;&gt;&gt; </span>trace = torch.jit.trace(compute, inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>print(trace.graph)
graph(%x<span class="hljs-number">.1</span> : Float(<span class="hljs-number">4</span>, <span class="hljs-number">8</span>)
 %y : Float(<span class="hljs-number">8</span>, <span class="hljs-number">5</span>)
 %z : Float(<span class="hljs-number">8</span>, <span class="hljs-number">5</span>)) {
 %<span class="hljs-number">3</span> : int = prim::Constant[value=<span class="hljs-number">3</span>]()
 %<span class="hljs-number">4</span> : int = prim::Constant[value=<span class="hljs-number">6</span>]()
 %<span class="hljs-number">5</span> : int = prim::Constant[value=<span class="hljs-number">0</span>]()
 %<span class="hljs-number">6</span> : int[] = prim::Constant[value=[<span class="hljs-number">0</span>, <span class="hljs-number">-1</span>]]()
 %<span class="hljs-number">7</span> : Float(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>) = aten::eye(%<span class="hljs-number">3</span>, %<span class="hljs-number">4</span>, %<span class="hljs-number">5</span>, %<span class="hljs-number">6</span>)
 %x : Float(<span class="hljs-number">8</span>, <span class="hljs-number">8</span>) = my_ops::warp_perspective(%x<span class="hljs-number">.1</span>, %<span class="hljs-number">7</span>)
 %<span class="hljs-number">11</span> : Float(<span class="hljs-number">8</span>, <span class="hljs-number">5</span>) = aten::matmul(%x, %y)
 %<span class="hljs-number">12</span> : Float(<span class="hljs-number">8</span>, <span class="hljs-number">5</span>) = aten::relu(%z)
 %<span class="hljs-number">13</span> : int = prim::Constant[value=<span class="hljs-number">1</span>]()
 %<span class="hljs-number">14</span> : Float(<span class="hljs-number">8</span>, <span class="hljs-number">5</span>) = aten::add(%<span class="hljs-number">11</span>, %<span class="hljs-number">12</span>, %<span class="hljs-number">13</span>)
 <span class="hljs-keyword">return</span> (%<span class="hljs-number">14</span>);
 }
</code></pre>
<p>Integrating TorchScript custom ops into traced PyTorch code is as easy as this!</p>
<h3 id="using-the-custom-operator-with-script">Using the Custom Operator with Script</h3>
<p>Besides tracing, another way to arrive at a TorchScript representation of a PyTorch program is to directly write your code <em>in</em> TorchScript. TorchScript is largely a subset of the Python language, with some restrictions that make it easier for the TorchScript compiler to reason about programs. You turn your regular PyTorch code into TorchScript by annotating it with <code>@torch.jit.script</code> for free functions and <code>@torch.jit.script_method</code> for methods in a class (which must also derive from <code>torch.jit.ScriptModule</code>). See <a href="https://pytorch.org/docs/master/jit.html" target="_blank">here</a> for more details on TorchScript annotations.</p>
<p>One particular reason to use TorchScript instead of tracing is that tracing is unable to capture control flow in PyTorch code. As such, let us consider this function which does use control flow:</p>
<pre><code class="lang-py"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">compute</span><span class="hljs-params">(x, y)</span>:</span>
  <span class="hljs-keyword">if</span> bool(x[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>] == <span class="hljs-number">42</span>):
      z = <span class="hljs-number">5</span>
  <span class="hljs-keyword">else</span>:
      z = <span class="hljs-number">10</span>
  <span class="hljs-keyword">return</span> x.matmul(y) + z
</code></pre>
<p>To convert this function from vanilla PyTorch to TorchScript, we annotate it with <code>@torch.jit.script</code>:</p>
<pre><code class="lang-py"><span class="hljs-meta">@torch.jit.script</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">compute</span><span class="hljs-params">(x, y)</span>:</span>
  <span class="hljs-keyword">if</span> bool(x[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>] == <span class="hljs-number">42</span>):
      z = <span class="hljs-number">5</span>
  <span class="hljs-keyword">else</span>:
      z = <span class="hljs-number">10</span>
  <span class="hljs-keyword">return</span> x.matmul(y) + z
</code></pre>
<p>This will just-in-time compile the <code>compute</code> function into a graph representation, which we can inspect in the <code>compute.graph</code> property:</p>
<pre><code class="lang-py">&gt;&gt;&gt; compute.graph
graph(%x : Dynamic
 %y : Dynamic) {
 %14 : int = prim::Constant[value=1]()
 %2 : int = prim::Constant[value=0]()
 %7 : int = prim::Constant[value=42]()
 %z.1 : int = prim::Constant[value=5]()
 %z.2 : int = prim::Constant[value=10]()
 %4 : Dynamic = aten::select(%x, %2, %2)
 %6 : Dynamic = aten::select(%4, %2, %2)
 %8 : Dynamic = aten::eq(%6, %7)
 %9 : bool = prim::TensorToBool(%8)
 %z : int = prim::If(%9)
 block0() {
 -&gt; (%z.1)
 }
 block1() {
 -&gt; (%z.2)
 }
 %13 : Dynamic = aten::matmul(%x, %y)
 %15 : Dynamic = aten::add(%13, %z, %14)
 return (%15);
}
</code></pre>
<p>And now, just like before, we can use our custom operator like any other function inside of our script code:</p>
<pre><code class="lang-py">torch.ops.load_library(<span class="hljs-string">&quot;libwarp_perspective.so&quot;</span>)

<span class="hljs-meta">@torch.jit.script</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">compute</span><span class="hljs-params">(x, y)</span>:</span>
  <span class="hljs-keyword">if</span> bool(x[<span class="hljs-number">0</span>] == <span class="hljs-number">42</span>):
      z = <span class="hljs-number">5</span>
  <span class="hljs-keyword">else</span>:
      z = <span class="hljs-number">10</span>
  x = torch.ops.my_ops.warp_perspective(x, torch.eye(<span class="hljs-number">3</span>))
  <span class="hljs-keyword">return</span> x.matmul(y) + z
</code></pre>
<p>When the TorchScript compiler sees the reference to <code>torch.ops.my_ops.warp_perspective</code>, it will find the implementation we registered via the <code>RegisterOperators</code> object in C++, and compile it into its graph representation:</p>
<pre><code class="lang-py">&gt;&gt;&gt; compute.graph
graph(%x.1 : Dynamic
 %y : Dynamic) {
 %20 : int = prim::Constant[value=1]()
 %16 : int[] = prim::Constant[value=[0, -1]]()
 %14 : int = prim::Constant[value=6]()
 %2 : int = prim::Constant[value=0]()
 %7 : int = prim::Constant[value=42]()
 %z.1 : int = prim::Constant[value=5]()
 %z.2 : int = prim::Constant[value=10]()
 %13 : int = prim::Constant[value=3]()
 %4 : Dynamic = aten::select(%x.1, %2, %2)
 %6 : Dynamic = aten::select(%4, %2, %2)
 %8 : Dynamic = aten::eq(%6, %7)
 %9 : bool = prim::TensorToBool(%8)
 %z : int = prim::If(%9)
 block0() {
 -&gt; (%z.1)
 }
 block1() {
 -&gt; (%z.2)
 }
 %17 : Dynamic = aten::eye(%13, %14, %2, %16)
 %x : Dynamic = my_ops::warp_perspective(%x.1, %17)
 %19 : Dynamic = aten::matmul(%x, %y)
 %21 : Dynamic = aten::add(%19, %z, %20)
 return (%21);
 }
</code></pre>
<p>Notice in particular the reference to <code>my_ops::warp_perspective</code> at the end of the graph.</p>
<p>Attention</p>
<p>The TorchScript graph representation is still subject to change. Do not rely on it looking like this.</p>
<p>And that&#x2019;s really it when it comes to using our custom operator in Python. In short, you import the library containing your operator(s) using <code>torch.ops.load_library</code>, and call your custom op like any other <code>torch</code> operator from your traced or scripted TorchScript code.</p>
<h2 id="using-the-torchscript-custom-operator-in-c">Using the TorchScript Custom Operator in C++</h2>
<p>One useful feature of TorchScript is the ability to serialize a model into an on-disk file. This file can be sent over the wire, stored in a file system or, more importantly, be dynamically deserialized and executed without needing to keep the original source code around. This is possible in Python, but also in C++. For this, PyTorch provides <a href="https://pytorch.org/cppdocs/" target="_blank">a pure C++ API</a> for deserializing as well as executing TorchScript models. If you haven&#x2019;t yet, please read <a href="https://pytorch.org/tutorials/advanced/cpp_export.html" target="_blank">the tutorial on loading and running serialized TorchScript models in C++</a>, on which the next few paragraphs will build.</p>
<p>In short, custom operators can be executed just like regular <code>torch</code> operators even when deserialized from a file and run in C++. The only requirement for this is to link the custom operator shared library we built earlier with the C++ application in which we execute the model. In Python, this worked simply calling <code>torch.ops.load_library</code>. In C++, you need to link the shared library with your main application in whatever build system you are using. The following example will showcase this using CMake.</p>
<p>Note</p>
<p>Technically, you can also dynamically load the shared library into your C++ application at runtime in much the same way we did it in Python. On Linux, <a href="https://tldp.org/HOWTO/Program-Library-HOWTO/dl-libraries.html" target="_blank">you can do this with dlopen</a>. There exist equivalents on other platforms.</p>
<p>Building on the C++ execution tutorial linked above, let&#x2019;s start with a minimal C++ application in one file, <code>main.cpp</code> in a different folder from our custom operator, that loads and executes a serialized TorchScript model:</p>
<pre><code class="lang-py">#include &lt;torch/script.h&gt; // One-stop header.

#include &lt;iostream&gt;
#include &lt;memory&gt;

int main(int argc, const char* argv[]) {
  if (argc != 2) {
    std::cerr &lt;&lt; &quot;usage: example-app &lt;path-to-exported-script-module&gt;\n&quot;;
    return -1;
  }

  // Deserialize the ScriptModule from a file using torch::jit::load().
  std::shared_ptr&lt;torch::jit::script::Module&gt; module = torch::jit::load(argv[1]);

  std::vector&lt;torch::jit::IValue&gt; inputs;
  inputs.push_back(torch::randn({4, 8}));
  inputs.push_back(torch::randn({8, 5}));

  torch::Tensor output = module-&gt;forward(std::move(inputs)).toTensor();

  std::cout &lt;&lt; output &lt;&lt; std::endl;
}
</code></pre>
<p>Along with a small <code>CMakeLists.txt</code> file:</p>
<pre><code class="lang-py">cmake_minimum_required(VERSION <span class="hljs-number">3.1</span> FATAL_ERROR)
project(example_app)

find_package(Torch REQUIRED)

add_executable(example_app main.cpp)
target_link_libraries(example_app <span class="hljs-string">&quot;${TORCH_LIBRARIES}&quot;</span>)
target_compile_features(example_app PRIVATE cxx_range_for)
</code></pre>
<p>At this point, we should be able to build the application:</p>
<pre><code class="lang-py">$ mkdir build
$ cd build
$ cmake -DCMAKE_PREFIX_PATH=/path/to/libtorch ..
-- The C compiler identification <span class="hljs-keyword">is</span> GNU <span class="hljs-number">5.4</span><span class="hljs-number">.0</span>
-- The CXX compiler identification <span class="hljs-keyword">is</span> GNU <span class="hljs-number">5.4</span><span class="hljs-number">.0</span>
-- Check <span class="hljs-keyword">for</span> working C compiler: /usr/bin/cc
-- Check <span class="hljs-keyword">for</span> working C compiler: /usr/bin/cc -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Detecting C compile features
-- Detecting C compile features - done
-- Check <span class="hljs-keyword">for</span> working CXX compiler: /usr/bin/c++
-- Check <span class="hljs-keyword">for</span> working CXX compiler: /usr/bin/c++ -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Looking <span class="hljs-keyword">for</span> pthread.h
-- Looking <span class="hljs-keyword">for</span> pthread.h - found
-- Looking <span class="hljs-keyword">for</span> pthread_create
-- Looking <span class="hljs-keyword">for</span> pthread_create - <span class="hljs-keyword">not</span> found
-- Looking <span class="hljs-keyword">for</span> pthread_create <span class="hljs-keyword">in</span> pthreads
-- Looking <span class="hljs-keyword">for</span> pthread_create <span class="hljs-keyword">in</span> pthreads - <span class="hljs-keyword">not</span> found
-- Looking <span class="hljs-keyword">for</span> pthread_create <span class="hljs-keyword">in</span> pthread
-- Looking <span class="hljs-keyword">for</span> pthread_create <span class="hljs-keyword">in</span> pthread - found
-- Found Threads: TRUE
-- Found torch: /libtorch/lib/libtorch.so
-- Configuring done
-- Generating done
-- Build files have been written to: /example_app/build
$ make -j
Scanning dependencies of target example_app
[ <span class="hljs-number">50</span>%] Building CXX object CMakeFiles/example_app.dir/main.cpp.o
[<span class="hljs-number">100</span>%] Linking CXX executable example_app
[<span class="hljs-number">100</span>%] Built target example_app
</code></pre>
<p>And run it without passing a model just yet:</p>
<pre><code class="lang-py">$ ./example_app
usage: example_app &lt;path-to-exported-script-module&gt;
</code></pre>
<p>Next, let&#x2019;s serialize the script function we wrote earlier that uses our custom operator:</p>
<pre><code class="lang-py">torch.ops.load_library(<span class="hljs-string">&quot;libwarp_perspective.so&quot;</span>)

<span class="hljs-meta">@torch.jit.script</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">compute</span><span class="hljs-params">(x, y)</span>:</span>
  <span class="hljs-keyword">if</span> bool(x[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>] == <span class="hljs-number">42</span>):
      z = <span class="hljs-number">5</span>
  <span class="hljs-keyword">else</span>:
      z = <span class="hljs-number">10</span>
  x = torch.ops.my_ops.warp_perspective(x, torch.eye(<span class="hljs-number">3</span>))
  <span class="hljs-keyword">return</span> x.matmul(y) + z

compute.save(<span class="hljs-string">&quot;example.pt&quot;</span>)
</code></pre>
<p>The last line will serialize the script function into a file called &#x201C;example.pt&#x201D;. If we then pass this serialized model to our C++ application, we can run it straight away:</p>
<pre><code class="lang-py">$ ./example_app example.pt
terminate called after throwing an instance of <span class="hljs-string">&apos;torch::jit::script::ErrorReport&apos;</span>
what():
Schema <span class="hljs-keyword">not</span> found <span class="hljs-keyword">for</span> node. File a bug report.
Node: %<span class="hljs-number">16</span> : Dynamic = my_ops::warp_perspective(%<span class="hljs-number">0</span>, %<span class="hljs-number">19</span>)
</code></pre>
<p>Or maybe not. Maybe not just yet. Of course! We haven&#x2019;t linked the custom operator library with our application yet. Let&#x2019;s do this right now, and to do it properly let&#x2019;s update our file organization slightly, to look like this:</p>
<pre><code class="lang-py">example_app/
  CMakeLists.txt
  main.cpp
  warp_perspective/
    CMakeLists.txt
    op.cpp
</code></pre>
<p>This will allow us to add the <code>warp_perspective</code> library CMake target as a subdirectory of our application target. The top level <code>CMakeLists.txt</code> in the <code>example_app</code> folder should look like this:</p>
<pre><code class="lang-py">cmake_minimum_required(VERSION <span class="hljs-number">3.1</span> FATAL_ERROR)
project(example_app)

find_package(Torch REQUIRED)

add_subdirectory(warp_perspective)

add_executable(example_app main.cpp)
target_link_libraries(example_app <span class="hljs-string">&quot;${TORCH_LIBRARIES}&quot;</span>)
target_link_libraries(example_app -Wl,--no-<span class="hljs-keyword">as</span>-needed warp_perspective)
target_compile_features(example_app PRIVATE cxx_range_for)
</code></pre>
<p>This basic CMake configuration looks much like before, except that we add the <code>warp_perspective</code> CMake build as a subdirectory. Once its CMake code runs, we link our <code>example_app</code> application with the <code>warp_perspective</code> shared library.</p>
<p>Attention</p>
<p>There is one crucial detail embedded in the above example: The <code>-Wl,--no-as-needed</code> prefix to the <code>warp_perspective</code> link line. This is required because we will not actually be calling any function from the <code>warp_perspective</code> shared library in our application code. We only need the global <code>RegisterOperators</code> object&#x2019;s constructor to run. Inconveniently, this confuses the linker and makes it think it can just skip linking against the library altogether. On Linux, the <code>-Wl,--no-as-needed</code> flag forces the link to happen (NB: this flag is specific to Linux!). There are other workarounds for this. The simplest is to define <em>some function</em> in the operator library that you need to call from the main application. This could be as simple as a function <code>void init();</code> declared in some header, which is then defined as <code>void init() { }</code> in the operator library. Calling this <code>init()</code> function in the main application will give the linker the impression that this is a library worth linking against. Unfortunately, this is outside of our control, and we would rather let you know the reason and the simple workaround for this than handing you some opaque macro to plop in your code.</p>
<p>Now, since we find the <code>Torch</code> package at the top level now, the <code>CMakeLists.txt</code> file in the <code>warp_perspective</code> subdirectory can be shortened a bit. It should look like this:</p>
<pre><code class="lang-py">find_package(OpenCV REQUIRED)
add_library(warp_perspective SHARED op.cpp)
target_compile_features(warp_perspective PRIVATE cxx_range_for)
target_link_libraries(warp_perspective PRIVATE <span class="hljs-string">&quot;${TORCH_LIBRARIES}&quot;</span>)
target_link_libraries(warp_perspective PRIVATE opencv_core opencv_photo)
</code></pre>
<p>Let&#x2019;s re-build our example app, which will also link with the custom operator library. In the top level <code>example_app</code> directory:</p>
<pre><code class="lang-py">$ mkdir build
$ cd build
$ cmake -DCMAKE_PREFIX_PATH=/path/to/libtorch ..
-- The C compiler identification <span class="hljs-keyword">is</span> GNU <span class="hljs-number">5.4</span><span class="hljs-number">.0</span>
-- The CXX compiler identification <span class="hljs-keyword">is</span> GNU <span class="hljs-number">5.4</span><span class="hljs-number">.0</span>
-- Check <span class="hljs-keyword">for</span> working C compiler: /usr/bin/cc
-- Check <span class="hljs-keyword">for</span> working C compiler: /usr/bin/cc -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Detecting C compile features
-- Detecting C compile features - done
-- Check <span class="hljs-keyword">for</span> working CXX compiler: /usr/bin/c++
-- Check <span class="hljs-keyword">for</span> working CXX compiler: /usr/bin/c++ -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Looking <span class="hljs-keyword">for</span> pthread.h
-- Looking <span class="hljs-keyword">for</span> pthread.h - found
-- Looking <span class="hljs-keyword">for</span> pthread_create
-- Looking <span class="hljs-keyword">for</span> pthread_create - <span class="hljs-keyword">not</span> found
-- Looking <span class="hljs-keyword">for</span> pthread_create <span class="hljs-keyword">in</span> pthreads
-- Looking <span class="hljs-keyword">for</span> pthread_create <span class="hljs-keyword">in</span> pthreads - <span class="hljs-keyword">not</span> found
-- Looking <span class="hljs-keyword">for</span> pthread_create <span class="hljs-keyword">in</span> pthread
-- Looking <span class="hljs-keyword">for</span> pthread_create <span class="hljs-keyword">in</span> pthread - found
-- Found Threads: TRUE
-- Found torch: /libtorch/lib/libtorch.so
-- Configuring done
-- Generating done
-- Build files have been written to: /warp_perspective/example_app/build
$ make -j
Scanning dependencies of target warp_perspective
[ <span class="hljs-number">25</span>%] Building CXX object warp_perspective/CMakeFiles/warp_perspective.dir/op.cpp.o
[ <span class="hljs-number">50</span>%] Linking CXX shared library libwarp_perspective.so
[ <span class="hljs-number">50</span>%] Built target warp_perspective
Scanning dependencies of target example_app
[ <span class="hljs-number">75</span>%] Building CXX object CMakeFiles/example_app.dir/main.cpp.o
[<span class="hljs-number">100</span>%] Linking CXX executable example_app
[<span class="hljs-number">100</span>%] Built target example_app
</code></pre>
<p>If we now run the <code>example_app</code> binary and hand it our serialized model, we should arrive at a happy ending:</p>
<pre><code class="lang-py">$ ./example_app example.pt
<span class="hljs-number">11.4125</span>   <span class="hljs-number">5.8262</span>   <span class="hljs-number">9.5345</span>   <span class="hljs-number">8.6111</span>  <span class="hljs-number">12.3997</span>
 <span class="hljs-number">7.4683</span>  <span class="hljs-number">13.5969</span>   <span class="hljs-number">9.0850</span>  <span class="hljs-number">11.0698</span>   <span class="hljs-number">9.4008</span>
 <span class="hljs-number">7.4597</span>  <span class="hljs-number">15.0926</span>  <span class="hljs-number">12.5727</span>   <span class="hljs-number">8.9319</span>   <span class="hljs-number">9.0666</span>
 <span class="hljs-number">9.4834</span>  <span class="hljs-number">11.1747</span>   <span class="hljs-number">9.0162</span>  <span class="hljs-number">10.9521</span>   <span class="hljs-number">8.6269</span>
<span class="hljs-number">10.0000</span>  <span class="hljs-number">10.0000</span>  <span class="hljs-number">10.0000</span>  <span class="hljs-number">10.0000</span>  <span class="hljs-number">10.0000</span>
<span class="hljs-number">10.0000</span>  <span class="hljs-number">10.0000</span>  <span class="hljs-number">10.0000</span>  <span class="hljs-number">10.0000</span>  <span class="hljs-number">10.0000</span>
<span class="hljs-number">10.0000</span>  <span class="hljs-number">10.0000</span>  <span class="hljs-number">10.0000</span>  <span class="hljs-number">10.0000</span>  <span class="hljs-number">10.0000</span>
<span class="hljs-number">10.0000</span>  <span class="hljs-number">10.0000</span>  <span class="hljs-number">10.0000</span>  <span class="hljs-number">10.0000</span>  <span class="hljs-number">10.0000</span>
[ Variable[CPUFloatType]{<span class="hljs-number">8</span>,<span class="hljs-number">5</span>} ]
</code></pre>
<p>Success! You are now ready to inference away.</p>
<h2 id="conclusion">Conclusion</h2>
<p>This tutorial walked you throw how to implement a custom TorchScript operator in C++, how to build it into a shared library, how to use it in Python to define TorchScript models and lastly how to load it into a C++ application for inference workloads. You are now ready to extend your TorchScript models with C++ operators that interface with third party C++ libraries, write custom high performance CUDA kernels, or implement any other use case that requires the lines between Python, TorchScript and C++ to blend smoothly.</p>
<p>As always, if you run into any problems or have questions, you can use our <a href="https://discuss.pytorch.org/" target="_blank">forum</a> or <a href="https://github.com/pytorch/pytorch/issues" target="_blank">GitHub issues</a> to get in touch. Also, our <a href="https://pytorch.org/cppdocs/notes/faq.html" target="_blank">frequently asked questions (FAQ) page</a> may have helpful information.</p>
<h2 id="appendix-a-more-ways-of-building-custom-operators">Appendix A: More Ways of Building Custom Operators</h2>
<p>The section &#x201C;Building the Custom Operator&#x201D; explained how to build a custom operator into a shared library using CMake. This appendix outlines two further approaches for compilation. Both of them use Python as the &#x201C;driver&#x201D; or &#x201C;interface&#x201D; to the compilation process. Also, both re-use the <a href="https://pytorch.org/docs/stable/cpp_extension.html" target="_blank">existing infrastructure</a> PyTorch provides for <a href="https://pytorch.org/tutorials/advanced/cpp_extension.html" target="_blank"><em>C++ extensions</em></a>, which are the vanilla (eager) PyTorch equivalent of TorchScript custom operators that rely on <a href="https://github.com/pybind/pybind11" target="_blank">pybind11</a> for &#x201C;explicit&#x201D; binding of functions from C++ into Python.</p>
<p>The first approach uses C++ extensions&#x2019; <a href="https://pytorch.org/docs/stable/cpp_extension.html#torch.utils.cpp_extension.load" target="_blank">convenient just-in-time (JIT) compilation interface</a> to compile your code in the background of your PyTorch script the first time you run it. The second approach relies on the venerable <code>setuptools</code> package and involves writing a separate <code>setup.py</code> file. This allows more advanced configuration as well as integration with other <code>setuptools</code>-based projects. We will explore both approaches in detail below.</p>
<h3 id="building-with-jit-compilation">Building with JIT compilation</h3>
<p>The JIT compilation feature provided by the PyTorch C++ extension toolkit allows embedding the compilation of your custom operator directly into your Python code, e.g. at the top of your training script.</p>
<p>Note</p>
<p>&#x201C;JIT compilation&#x201D; here has nothing to do with the JIT compilation taking place in the TorchScript compiler to optimize your program. It simply means that your custom operator C++ code will be compiled in a folder under your system&#x2019;s <code>/tmp</code> directory the first time you import it, as if you had compiled it yourself beforehand.</p>
<p>This JIT compilation feature comes in two flavors. In the first, you still keep your operator implementation in a separate file (<code>op.cpp</code>), and then use <code>torch.utils.cpp_extension.load()</code> to compile your extension. Usually, this function will return the Python module exposing your C++ extension. However, since we are not compiling our custom operator into its own Python module, we only want to compile a plain shared library . Fortunately, <code>torch.utils.cpp_extension.load()</code> has an argument <code>is_python_module</code> which we can set to <code>False</code> to indicate that we are only interested in building a shared library and not a Python module. <code>torch.utils.cpp_extension.load()</code> will then compile and also load the shared library into the current process, just like <code>torch.ops.load_library</code> did before:</p>
<pre><code class="lang-py"><span class="hljs-keyword">import</span> torch.utils.cpp_extension

torch.utils.cpp_extension.load(
    name=<span class="hljs-string">&quot;warp_perspective&quot;</span>,
    sources=[<span class="hljs-string">&quot;op.cpp&quot;</span>],
    extra_ldflags=[<span class="hljs-string">&quot;-lopencv_core&quot;</span>, <span class="hljs-string">&quot;-lopencv_imgproc&quot;</span>],
    is_python_module=<span class="hljs-keyword">False</span>,
    verbose=<span class="hljs-keyword">True</span>
)

print(torch.ops.my_ops.warp_perspective)
</code></pre>
<p>This should approximately print:</p>
<pre><code class="lang-py">&lt;built-<span class="hljs-keyword">in</span> method my_ops::warp_perspective of PyCapsule object at <span class="hljs-number">0x7f3e0f840b10</span>&gt;
</code></pre>
<p>The second flavor of JIT compilation allows you to pass the source code for your custom TorchScript operator as a string. For this, use <code>torch.utils.cpp_extension.load_inline</code>:</p>
<pre><code class="lang-py"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torch.utils.cpp_extension

op_source = <span class="hljs-string">&quot;&quot;&quot;
#include &lt;opencv2/opencv.hpp&gt;
#include &lt;torch/script.h&gt;

torch::Tensor warp_perspective(torch::Tensor image, torch::Tensor warp) {
 cv::Mat image_mat(/*rows=*/image.size(0),
 /*cols=*/image.size(1),
 /*type=*/CV_32FC1,
 /*data=*/image.data&lt;float&gt;());
 cv::Mat warp_mat(/*rows=*/warp.size(0),
 /*cols=*/warp.size(1),
 /*type=*/CV_32FC1,
 /*data=*/warp.data&lt;float&gt;());

 cv::Mat output_mat;
 cv::warpPerspective(image_mat, output_mat, warp_mat, /*dsize=*/{64, 64});

 torch::Tensor output =
 torch::from_blob(output_mat.ptr&lt;float&gt;(), /*sizes=*/{64, 64});
 return output.clone();
}

static auto registry =
 torch::jit::RegisterOperators(&quot;my_ops::warp_perspective&quot;, &amp;warp_perspective);
&quot;&quot;&quot;</span>

torch.utils.cpp_extension.load_inline(
    name=<span class="hljs-string">&quot;warp_perspective&quot;</span>,
    cpp_sources=op_source,
    extra_ldflags=[<span class="hljs-string">&quot;-lopencv_core&quot;</span>, <span class="hljs-string">&quot;-lopencv_imgproc&quot;</span>],
    is_python_module=<span class="hljs-keyword">False</span>,
    verbose=<span class="hljs-keyword">True</span>,
)

print(torch.ops.my_ops.warp_perspective)
</code></pre>
<p>Naturally, it is best practice to only use <code>torch.utils.cpp_extension.load_inline</code> if your source code is reasonably short.</p>
<h3 id="building-with-setuptools">Building with Setuptools</h3>
<p>The second approach to building our custom operator exclusively from Python is to use <code>setuptools</code>. This has the advantage that <code>setuptools</code> has a quite powerful and extensive interface for building Python modules written in C++. However, since <code>setuptools</code> is really intended for building Python modules and not plain shared libraries (which do not have the necessary entry points Python expects from a module), this route can be slightly quirky. That said, all you need is a <code>setup.py</code> file in place of the <code>CMakeLists.txt</code> which looks like this:</p>
<p>Notice that we enabled the <code>no_python_abi_suffix</code> option in the <code>BuildExtension</code> at the bottom. This instructs <code>setuptools</code> to omit any Python-3 specific ABI suffixes in the name of the produced shared library. Otherwise, on Python 3.7 for example, the library may be called <code>warp_perspective.cpython-37m-x86_64-linux-gnu.so</code> where <code>cpython-37m-x86_64-linux-gnu</code> is the ABI tag, but we really just want it to be called <code>warp_perspective.so</code></p>
<p>If we now run <code>python setup.py build develop</code> in a terminal from within the folder in which <code>setup.py</code> is situated, we should see something like:</p>
<pre><code class="lang-py">$ python setup.py build develop
running build
running build_ext
building &apos;warp_perspective&apos; extension
creating build
creating build/temp.linux-x86_64-3.7
gcc -pthread -B /root/local/miniconda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/root/local/miniconda/lib/python3.7/site-packages/torch/lib/include -I/root/local/miniconda/lib/python3.7/site-packages/torch/lib/include/torch/csrc/api/include -I/root/local/miniconda/lib/python3.7/site-packages/torch/lib/include/TH -I/root/local/miniconda/lib/python3.7/site-packages/torch/lib/include/THC -I/root/local/miniconda/include/python3.7m -c op.cpp -o build/temp.linux-x86_64-3.7/op.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=warp_perspective -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11
cc1plus: warning: command line option &#x2018;-Wstrict-prototypes&#x2019; is valid for C/ObjC but not for C++
creating build/lib.linux-x86_64-3.7
g++ -pthread -shared -B /root/local/miniconda/compiler_compat -L/root/local/miniconda/lib -Wl,-rpath=/root/local/miniconda/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.7/op.o -lopencv_core -lopencv_imgproc -o build/lib.linux-x86_64-3.7/warp_perspective.so
running develop
running egg_info
creating warp_perspective.egg-info
writing warp_perspective.egg-info/PKG-INFO
writing dependency_links to warp_perspective.egg-info/dependency_links.txt
writing top-level names to warp_perspective.egg-info/top_level.txt
writing manifest file &apos;warp_perspective.egg-info/SOURCES.txt&apos;
reading manifest file &apos;warp_perspective.egg-info/SOURCES.txt&apos;
writing manifest file &apos;warp_perspective.egg-info/SOURCES.txt&apos;
running build_ext
copying build/lib.linux-x86_64-3.7/warp_perspective.so -&gt;
Creating /root/local/miniconda/lib/python3.7/site-packages/warp-perspective.egg-link (link to .)
Adding warp-perspective 0.0.0 to easy-install.pth file

Installed /warp_perspective
Processing dependencies for warp-perspective==0.0.0
Finished processing dependencies for warp-perspective==0.0.0
</code></pre>
<p>This will produce a shared library called <code>warp_perspective.so</code>, which we can pass to <code>torch.ops.load_library</code> as we did earlier to make our operator visible to TorchScript:</p>
<footer class="page-footer"><span class="copyright">Copyright &#xA9; ibooker.org.cn 2019 all right reserved&#xFF0C;&#x7531; ApacheCN &#x56E2;&#x961F;&#x63D0;&#x4F9B;&#x652F;&#x6301;</span><span class="footer-modification">&#x8BE5;&#x6587;&#x4EF6;&#x4FEE;&#x8BA2;&#x65F6;&#x95F4;&#xFF1A; 
2019-06-17 07:36:30
</span></footer>
<script>console.log("plugin-popup....");document.onclick = function(e){ e.target.tagName === "IMG" && window.open(e.target.src,e.target.src)}</script><style>img{cursor:pointer}</style>
                                
                                </section>
                            
    </div>
    <div class="search-results">
        <div class="has-results">
            
            <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
            <ul class="search-results-list"></ul>
            
        </div>
        <div class="no-results">
            
            <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
            
        </div>
    </div>
</div>

                        </div>
                    </div>
                
            </div>

            
                
                <a href="cpp_extension.html" class="navigation navigation-prev " aria-label="Previous page: Custom C++   and CUDA Extensions">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="tut_production_usage.html" class="navigation navigation-next " aria-label="Next page: 生产性使用">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"Extending TorchScript with Custom C++   Operators","level":"1.2.6.3","depth":3,"next":{"title":"生产性使用","level":"1.2.7","depth":2,"path":"tut_production_usage.md","ref":"tut_production_usage.md","articles":[{"title":"Writing Distributed Applications with PyTorch","level":"1.2.7.1","depth":3,"path":"dist_tuto.md","ref":"dist_tuto.md","articles":[]},{"title":"使用 Amazon AWS 进行分布式训练","level":"1.2.7.2","depth":3,"path":"aws_distributed_training_tutorial.md","ref":"aws_distributed_training_tutorial.md","articles":[]},{"title":"ONNX 现场演示教程","level":"1.2.7.3","depth":3,"path":"ONNXLive.md","ref":"ONNXLive.md","articles":[]},{"title":"在 C++ 中加载 PYTORCH 模型","level":"1.2.7.4","depth":3,"path":"cpp_export.md","ref":"cpp_export.md","articles":[]}]},"previous":{"title":"Custom C++   and CUDA Extensions","level":"1.2.6.2","depth":3,"path":"cpp_extension.md","ref":"cpp_extension.md","articles":[]},"dir":"ltr"},"config":{"plugins":["github","-sharing","insert-logo","sharing-plus","back-to-top-button","code","copy-code-button","mathjax","pageview-count","edit-link","emphasize","alerts","auto-scroll-table","popup","hide-element","page-toc-button","github-buttons","tbfed-pagefooter"],"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"pluginsConfig":{"tbfed-pagefooter":{"copyright":"Copyright &copy ibooker.org.cn 2019","modify_label":"该文件修订时间： ","modify_format":"YYYY-MM-DD HH:mm:ss"},"emphasize":{},"github":{"url":"https://github.com/apachecn"},"search":{},"sharing-plus":{"qq":false,"all":["facebook","google","twitter","instapaper","linkedin","pocket","stumbleupon"],"douban":false,"facebook":true,"weibo":false,"instapaper":false,"whatsapp":false,"hatenaBookmark":false,"twitter":true,"messenger":false,"line":false,"vk":false,"pocket":true,"google":false,"viber":false,"stumbleupon":false,"qzone":false,"linkedin":false},"auto-scroll-table":{},"popup":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"code":{"copyButtons":true},"hide-element":{"elements":[".gitbook-link"]},"fontsettings":{"theme":"white","family":"sans","size":2},"highlight":{},"page-toc-button":{"maxTocDepth":4,"minTocSize":4},"back-to-top-button":{},"pageview-count":{},"alerts":{},"github-buttons":{},"mathjax":{"forceSVG":false,"version":"2.6-latest"},"copy-code-button":{},"sharing":{"qq":false,"all":["qq","douban","facebook","google","linkedin","twitter","weibo","whatsapp"],"douban":false,"facebook":false,"weibo":true,"whatsapp":false,"twitter":false,"line":false,"google":false,"qzone":true},"edit-link":{"label":"编辑本页","base":"https://github.com/apachecn/pytorch-doc-zh/blob/master"},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false},"insert-logo":{"style":"background: none; max-height: 150px; min-height: 150px","url":"http://data.apachecn.org/img/logo.jpg"}},"my_links":{"sidebar":{"Home":"https://www.baidu.com"}},"theme":"default","author":"ApacheCN","my_plugins":["splitter","donate","search-pro","todo","-lunr","-search","expandable-chapters-small","chapter-fold","expandable-chapters","expandable-chapters-small","back-to-top-button","advanced-emoji","tbfed-pagefooter","sectionx","page-treeview","simple-page-toc","github-buttons","ancre-navigation"],"my_pluginsConfig":{"ignores":["node_modules"],"simple-page-toc":{"maxDepth":3,"skipFirstH1":true},"page-toc-button":{"maxTocDepth":2,"minTocSize":2},"page-treeview":{"copyright":"Copyright &#169; aleen42","minHeaderCount":"2","minHeaderDeep":"2"},"donate":{"wechat":"微信收款的二维码URL","alipay":"支付宝收款的二维码URL","title":"","button":"赏","alipayText":"支付宝打赏","wechatText":"微信打赏"},"page-copyright":{"wisdom":"Designer, Frontend Developer & overall web enthusiast","noPowered":false,"copyright":"Copyright &#169; 你的名字","style":"normal","timeColor":"#666","utcOffset":"8","format":"YYYY-MM-dd hh:mm:ss","signature":"你的签名","copyrightColor":"#666","description":"modified at"}},"pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"variables":{},"title":"Pytorch 中文文档","language":"zh-hans","gitbook":"*","description":"Pytorch 中文文档: 教程和文档"},"file":{"path":"torch_script_custom_ops.md","mtime":"2019-06-17T07:36:30.752Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2019-06-17T07:38:27.820Z"},"basePath":".","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="gitbook/gitbook.js"></script>
    <script src="gitbook/theme.js"></script>
    
        
        <script src="gitbook/gitbook-plugin-github/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-insert-logo/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-sharing-plus/buttons.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-back-to-top-button/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-code/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-copy-code-button/toggle.js"></script>
        
    
        
        <script src="https://cdn.mathjax.org/mathjax/2.6-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-mathjax/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-pageview-count/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-edit-link/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-alerts/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-auto-scroll-table/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-hide-element/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-page-toc-button/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-github-buttons/plugin.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-search/search-engine.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-search/search.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>

